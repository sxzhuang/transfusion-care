{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1QxFdHaG6jgY",
        "outputId": "5bff1908-9bdf-4e0f-fcf6-23f7645b1b64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from scipy.io import loadmat\n",
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import numpy as np\n",
        "from itertools import product, combinations\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Sequence, Tuple, Dict, Any\n",
        "\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8RlPjiDAELoI"
      },
      "source": [
        "# Generating Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "d4yM3X4mD-Ay",
        "outputId": "0289b43d-5811-4443-b821-11eda01fc9a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Alignment diagnostics (matching by (patient_id, case_id) as strings) ===\n",
            "{'intra_unique_keys': 26904, 'post_unique_keys': 26955, 'intersection_keys': 26904, 'missing_in_post_y_count': 0, 'missing_in_intra_count': 51, 'missing_in_post_y_sample': [], 'missing_in_intra_sample': [('BAH0014070', 'BLH0013253'), ('BAH0006661', 'BLH0015509'), ('BAH0017194', 'BLH0016503'), ('BAH0018845', 'BLH0018407'), ('BAH0008319', 'BLH0006486'), ('BAH0012894', 'BLH0011512'), ('BAH0009868', 'BLH0015342'), ('BAH0020671', 'BLH0020631'), ('BAH0016512', 'BLH0015639'), ('BAH0016219', 'BLH0015300')]}\n",
            "\n",
            "=== Post_y diagnostics ===\n",
            "{'post_y_rows': 26955, 'post_y_unique_keys': 26955, 'post_y_duplicate_keys_count': 0, 'post_y_conflicting_label_keys_count': 0, 'post_y_conflicting_label_sample': []}\n",
            "\n",
            "=== Pre_v diagnostics ===\n",
            "{'pre_v_rows': 26955, 'pre_v_unique_keys': 26955, 'pre_v_duplicate_keys_count': 0, 'pre_v_duplicate_keys_sample': []}\n",
            "\n",
            "=== Intra_v diagnostics ===\n",
            "{'intra_v_rows': 26955, 'intra_v_unique_keys': 26955, 'intra_v_duplicate_keys_count': 0, 'intra_v_duplicate_keys_sample': []}\n",
            "\n",
            "=== Intra_xa diagnostics ===\n",
            "{'intra_rows': 202240, 'intra_unique_keys': 26904, 'total_surgeries_in_intra': 26904, 'kept_aligned': 26904, 'dropped_unlabeled': 0, 'cumulative_like_cols': ['CPBresidualBlood_culVolume', 'intraoper_Autoblood_culVolume', 'intraoper_GCs_culmulative_value', 'intraoper_Plasma_culVolume', 'intraoper_Platelets_culVolume', 'intraoper_colloid_culVolume', 'intraoper_cumulative_HRV', 'intraoper_cumulative_MAP_auc', 'intraoper_cumulative_hypothermia_auc', 'intraoper_cumulative_hypoxic_auc'], 'binary_cols_detected': ['intraoper_Esmolol_used', 'intraoper_Inhalation_anesthetics', 'intraoper_Ketamine_value', 'intraoper_Propofol_used', 'intraoper_Terlipressin_used', 'isultrafilter_used'], 'negative_x_cols_raw': [], 'negative_x_cols_post': []}\n",
            "\n",
            "=== Static v diagnostics ===\n",
            "{'static_dim_v': 91, 'missing_pre_v_count_in_intra': 0, 'missing_intra_v_count_in_intra': 0, 'pre_v_unique_keys': 26955, 'intra_v_unique_keys': 26955}\n",
            "ASA encoding: onehot | ethnicity categories (train-fitted): 3\n",
            "dim_v = 91\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== X normalization lists (based on TRAIN split stats) ===\n",
            "x <- x / std (scale-only):\n",
            "['intraoper_cumulative_HRV', 'intraoper_cumulative_hypothermia_auc', 'intraoper_cumulative_hypoxic_auc', 'intraoper_cumulative_MAP_auc', 'intraoper_FIO2_ArtBGA', 'intraoper_Oxygenation_index', 'intraoper_tHb_value', 'intraoper_BE_value', 'intraoper_cHCO3_value', 'intraoper_CVP_value', 'intraoper_Ca_value', 'intraoper_Na_value', 'intraoper_K_value', 'intraoper_Glu_value', 'intraoper_Lac_value', 'intraoper_PH_value', 'intraoper_Norepinephrine_max', 'intraoper_Dopamine_max', 'intraoper_Dobutamine_max', 'intraoper_Isoproterenol_max', 'intraoper_Adrenaline_max', 'intraoper_Milrinone_max', 'intraoper_Baquting_value', 'intraoper_Tranexamic_Acid_value', 'intraoper_Heparin_value', 'intraoper_protamine_value', 'intraoper_opioids_MME_value', 'intraoper_GCs_culmulative_value', 'intraoper_Platelets_culVolume', 'intraoper_Plasma_culVolume', 'intraoper_Autoblood_culVolume', 'CPBresidualBlood_culVolume', 'intraoper_colloid_culVolume']\n",
            "\n",
            "x <- (x - mean) / std (z-score):\n",
            "[]\n",
            "\n",
            "(no normalization):\n",
            "['intraoper_Terlipressin_used', 'intraoper_Esmolol_used', 'intraoper_Ketamine_value', 'intraoper_Inhalation_anesthetics', 'intraoper_Propofol_used', 'isultrafilter_used']\n",
            "\n",
            "=== Negative-value x columns ===\n",
            "Negative values in RAW x (before cumulative->incremental):\n",
            "[]\n",
            "\n",
            "Negative values AFTER your chosen representation:\n",
            "[]\n",
            "\n",
            "=== V normalization lists (based on TRAIN split stats) ===\n",
            "v <- v / std (scale-only):\n",
            "['adm_age', 'BMI', 'preoper_DBP', 'preoper_Heart_Rate', 'preoper_Pulse', 'preoper_SBP', 'preoper_Temperature', 'preoper_Respiratory_Rate', 'd_NYHA_Level', 'd_arrhy_avb', 'd_hyper_hypo_thyroidism', 'preoperLab_NT_proBNP', 'preoperLab_CRP', 'preoperLab_Neutrophil_Percentage', 'preoperLab_eGFR', 'preoperLab_PT', 'preoperLab_TBIL', 'preoperLab_APTT', 'preoperLab_WBC_Count', 'preoperLab_ALB', 'preoperLab_RBC_Count', 'preoperLab_TnT', 'preoperLab_Cystatin_C', 'preoperLab_Platelet_Count', 'preoperLab_BUN', 'preoperLab_ESR', 'preoperLab_Hemoglobin', 'preoperLab_Glucose', 'preoper_LVEF']\n",
            "\n",
            "v <- (v - mean) / std (z-score):\n",
            "[]\n",
            "\n",
            "(no normalization):\n",
            "['gender', 'cardiac_surgery_history_adm', 'renal_surgery_history_adm', 'allergy_history_adm', 'drinking_history_adm', 'smoking_history_adm', 'transfusion_history_adm', 'd_Valve_Dis', 'd_Rheumatic_HD', 'd_Congenital_HD', 'd_Aortic_related_dis', 'd_Coronary_HD', 'd_Cardiac_Tumor', 'd_Cardiomyopathy', 'd_Infect_Endocarditis', 'd_Pericardial_Dis', 'd_Liver_Disease', 'd_CKD_Status', 'd_Cerebrovascular_Events', 'd_AF_af_Arrhythmia', 'd_arrhy_cp_icd_crt', 'd_auto_immune', 'd_copd', 'd_dm', 'd_lipn', 'd_pvd', 'd_sepsis', 'd_htn', 'd_pulmonary_hypertension', 'preoperDrug_Glucocorticoid_Usage', 'preoperDrug_Amphotericin_Aminoglycoside_Combined_Usage', 'preoperDrug_ACEIARB_Usage', 'preoperDrug_Diuretics_Usage', 'preoperDrug_NSAIDs_Usage', 'preoperDrug_Norepinephrine', 'preoperDrug_vasopressinUSE', 'preoperDrug_Dopamine', 'preoperDrug_Nitroprusside', 'preoperDrug_Dobutamine', 'preoperDrug_Isoproterenol', 'preoperDrug_Epinephrine', 'preoperDrug_Nitroglycerin', 'preoperDrug_Metaraminol', 'preoperDrug_Contrast_Exposure', 'pre_aki', 'ethnicity__1.0', 'ethnicity__2.0', 'ethnicity__3.0', 'ethnicity__UNK', 'is_emergency', 'CABG_oper', 'CardiacTumor_oper', 'HeartTransplant_oper', 'aortic_oper', 'congenital_oper', 'valve_oper', 'ASA_class__1', 'ASA_class__2', 'ASA_class__3', 'ASA_class__4', 'ASA_class__5', 'ASA_class__UNK']\n",
            "\n",
            "=== Negative-value v columns (on TRAIN matrix before normalization) ===\n",
            "[]\n",
            "\n",
            "=== Batch shapes ===\n",
            "x_pad: (32, 15, 39) a_pad: (32, 15) y: (32, 1) v: (32, 91)\n",
            "lengths: (32,) mask: (32, 15)\n"
          ]
        }
      ],
      "source": [
        "# transfusion_loaders_with_static_v.py\n",
        "from __future__ import annotations\n",
        "from dataclasses import dataclass\n",
        "from typing import List, Optional, Tuple, Dict, Any, Set, Sequence\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Paths\n",
        "# ============================================================\n",
        "INTRA_XA_PATH = \"/content/drive/MyDrive/coding/Dataset/transfusion/Intra_xa.csv\"\n",
        "POST_Y_PATH   = \"/content/drive/MyDrive/coding/Dataset/transfusion/Post_y.csv\"\n",
        "PRE_V_PATH    = \"/content/drive/MyDrive/coding/Dataset/transfusion/Pre_v.csv\"\n",
        "INTRA_V_PATH  = \"/content/drive/MyDrive/coding/Dataset/transfusion/Intra_v.csv\"\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Column names\n",
        "# ============================================================\n",
        "PATIENT_ID_COL = \"patient_id\"\n",
        "CASE_ID_COL    = \"case_id\"\n",
        "TIME_COL       = \"intraoper_time_periods\"\n",
        "ACTION_CUM_COL = \"intraoper_pRBC_culVolume\"\n",
        "Y_LABEL_COL    = \"AKI_class_occur\"\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Intra x columns (39)\n",
        "# ============================================================\n",
        "X_COLS: List[str] = [\n",
        "    \"intraoper_cumulative_HRV\",\n",
        "    \"intraoper_cumulative_hypothermia_auc\",\n",
        "    \"intraoper_cumulative_hypoxic_auc\",\n",
        "    \"intraoper_cumulative_MAP_auc\",\n",
        "    \"intraoper_FIO2_ArtBGA\",\n",
        "    \"intraoper_Oxygenation_index\",\n",
        "    \"intraoper_tHb_value\",\n",
        "    \"intraoper_BE_value\",\n",
        "    \"intraoper_cHCO3_value\",\n",
        "    \"intraoper_CVP_value\",\n",
        "    \"intraoper_Ca_value\",\n",
        "    \"intraoper_Na_value\",\n",
        "    \"intraoper_K_value\",\n",
        "    \"intraoper_Glu_value\",\n",
        "    \"intraoper_Lac_value\",\n",
        "    \"intraoper_PH_value\",\n",
        "    \"intraoper_Norepinephrine_max\",\n",
        "    \"intraoper_Dopamine_max\",\n",
        "    \"intraoper_Dobutamine_max\",\n",
        "    \"intraoper_Isoproterenol_max\",\n",
        "    \"intraoper_Adrenaline_max\",\n",
        "    \"intraoper_Milrinone_max\",\n",
        "    \"intraoper_Terlipressin_used\",\n",
        "    \"intraoper_Esmolol_used\",\n",
        "    \"intraoper_Baquting_value\",\n",
        "    \"intraoper_Tranexamic_Acid_value\",\n",
        "    \"intraoper_Ketamine_value\",\n",
        "    \"intraoper_Heparin_value\",\n",
        "    \"intraoper_protamine_value\",\n",
        "    \"intraoper_opioids_MME_value\",\n",
        "    \"intraoper_GCs_culmulative_value\",\n",
        "    \"intraoper_Platelets_culVolume\",\n",
        "    \"intraoper_Plasma_culVolume\",\n",
        "    \"intraoper_Autoblood_culVolume\",\n",
        "    \"CPBresidualBlood_culVolume\",\n",
        "    \"intraoper_Inhalation_anesthetics\",\n",
        "    \"intraoper_Propofol_used\",\n",
        "    \"intraoper_colloid_culVolume\",\n",
        "    \"isultrafilter_used\",\n",
        "]\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Static v columns\n",
        "# ============================================================\n",
        "PRE_V_COLS: List[str] = [\n",
        "    \"gender\",  # 男/女\n",
        "    \"adm_age\",\n",
        "    \"ethnicity\",\n",
        "    \"BMI\",\n",
        "    \"cardiac_surgery_history_adm\",\n",
        "    \"renal_surgery_history_adm\",\n",
        "    \"allergy_history_adm\",\n",
        "    \"drinking_history_adm\",\n",
        "    \"smoking_history_adm\",\n",
        "    \"transfusion_history_adm\",\n",
        "    \"preoper_DBP\",\n",
        "    \"preoper_Heart_Rate\",\n",
        "    \"preoper_Pulse\",\n",
        "    \"preoper_SBP\",\n",
        "    \"preoper_Temperature\",\n",
        "    \"preoper_Respiratory_Rate\",\n",
        "    \"d_Valve_Dis\",\n",
        "    \"d_Rheumatic_HD\",\n",
        "    \"d_Congenital_HD\",\n",
        "    \"d_Aortic_related_dis\",\n",
        "    \"d_Coronary_HD\",\n",
        "    \"d_Cardiac_Tumor\",\n",
        "    \"d_Cardiomyopathy\",\n",
        "    \"d_Infect_Endocarditis\",\n",
        "    \"d_Pericardial_Dis\",\n",
        "    \"d_Liver_Disease\",\n",
        "    \"d_CKD_Status\",\n",
        "    \"d_NYHA_Level\",\n",
        "    \"d_Cerebrovascular_Events\",\n",
        "    \"d_AF_af_Arrhythmia\",\n",
        "    \"d_arrhy_avb\",\n",
        "    \"d_arrhy_cp_icd_crt\",\n",
        "    \"d_auto_immune\",\n",
        "    \"d_copd\",\n",
        "    \"d_dm\",\n",
        "    \"d_hyper_hypo_thyroidism\",\n",
        "    \"d_lipn\",\n",
        "    \"d_pvd\",\n",
        "    \"d_sepsis\",\n",
        "    \"d_htn\",\n",
        "    \"d_pulmonary_hypertension\",\n",
        "    \"preoperLab_NT_proBNP\",\n",
        "    \"preoperLab_CRP\",\n",
        "    \"preoperLab_Neutrophil_Percentage\",\n",
        "    \"preoperLab_eGFR\",\n",
        "    \"preoperLab_PT\",\n",
        "    \"preoperLab_TBIL\",\n",
        "    \"preoperLab_APTT\",\n",
        "    \"preoperLab_WBC_Count\",\n",
        "    \"preoperLab_ALB\",\n",
        "    \"preoperLab_RBC_Count\",\n",
        "    \"preoperLab_TnT\",\n",
        "    \"preoperLab_Cystatin_C\",\n",
        "    \"preoperLab_Platelet_Count\",\n",
        "    \"preoperLab_BUN\",\n",
        "    \"preoperLab_ESR\",\n",
        "    \"preoperLab_Hemoglobin\",\n",
        "    \"preoperLab_Glucose\",\n",
        "    \"preoper_LVEF\",\n",
        "    \"preoperDrug_Glucocorticoid_Usage\",\n",
        "    \"preoperDrug_Amphotericin_Aminoglycoside_Combined_Usage\",\n",
        "    \"preoperDrug_ACEIARB_Usage\",\n",
        "    \"preoperDrug_Diuretics_Usage\",\n",
        "    \"preoperDrug_NSAIDs_Usage\",\n",
        "    \"preoperDrug_Norepinephrine\",\n",
        "    \"preoperDrug_vasopressinUSE\",\n",
        "    \"preoperDrug_Dopamine\",\n",
        "    \"preoperDrug_Nitroprusside\",\n",
        "    \"preoperDrug_Dobutamine\",\n",
        "    \"preoperDrug_Isoproterenol\",\n",
        "    \"preoperDrug_Epinephrine\",\n",
        "    \"preoperDrug_Nitroglycerin\",\n",
        "    \"preoperDrug_Metaraminol\",\n",
        "    \"preoperDrug_Contrast_Exposure\",\n",
        "    \"pre_aki\",\n",
        "]\n",
        "\n",
        "INTRA_V_COLS: List[str] = [\n",
        "    \"is_emergency\",  # 1 non-emergency, 2 emergency (binary)\n",
        "    \"ASA_class\",     # 1..5 (we support ordinal or onehot)\n",
        "    \"CABG_oper\",\n",
        "    \"CardiacTumor_oper\",\n",
        "    \"HeartTransplant_oper\",\n",
        "    \"aortic_oper\",\n",
        "    \"congenital_oper\",\n",
        "    \"valve_oper\",\n",
        "]\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# ID cleaning\n",
        "# ============================================================\n",
        "def _clean_id_series(s: pd.Series) -> pd.Series:\n",
        "    s = s.astype(str).str.strip()\n",
        "    s = s.str.replace(r\"\\.0$\", \"\", regex=True)\n",
        "    s = s.replace({\"nan\": \"\", \"None\": \"\", \"<NA>\": \"\"})\n",
        "    return s\n",
        "\n",
        "def normalize_patient_id(s: pd.Series, width: Optional[int] = None) -> pd.Series:\n",
        "    s = _clean_id_series(s)\n",
        "    if width is None:\n",
        "        return s\n",
        "    is_digits = s.str.fullmatch(r\"\\d+\").fillna(False)\n",
        "    s.loc[is_digits] = s.loc[is_digits].str.zfill(width)\n",
        "    return s\n",
        "\n",
        "def normalize_case_id(s: pd.Series) -> pd.Series:\n",
        "    return _clean_id_series(s)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Action bins\n",
        "# ============================================================\n",
        "_ACTION_BINS = np.array([0, 1, 2, 3, 4, 5], dtype=np.float32)\n",
        "\n",
        "def bin_incremental_action(delta: np.ndarray) -> np.ndarray:\n",
        "    delta = np.asarray(delta, dtype=np.float32)\n",
        "    delta = np.clip(delta, 0.0, np.inf)\n",
        "    return np.digitize(delta, bins=_ACTION_BINS, right=True).astype(np.int64)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Collate: now returns x, a, y, v, lengths, mask\n",
        "# ============================================================\n",
        "def pad_collate_varlen(batch, pad_action: int = 0):\n",
        "    xs, aas, ys, vs, lens = zip(*batch)\n",
        "    lengths = torch.stack(lens, dim=0)\n",
        "    B = len(batch)\n",
        "    T_max = int(lengths.max().item())\n",
        "    x_dim = xs[0].shape[-1]\n",
        "    v_dim = vs[0].shape[-1]\n",
        "\n",
        "    x_pad = torch.zeros(B, T_max, x_dim, dtype=torch.float32)\n",
        "    a_pad = torch.full((B, T_max), fill_value=int(pad_action), dtype=torch.long)\n",
        "\n",
        "    # FIX: y is (B,1) to match the simulator + your model assumptions\n",
        "    y = torch.stack(ys, dim=0).to(dtype=torch.float32).view(B, 1)\n",
        "\n",
        "    v = torch.stack(vs, dim=0).to(dtype=torch.float32).view(B, v_dim)\n",
        "\n",
        "    for i, (x_i, a_i, L) in enumerate(zip(xs, aas, lengths)):\n",
        "        L = int(L.item())\n",
        "        x_pad[i, :L, :] = x_i\n",
        "        a_pad[i, :L] = a_i\n",
        "\n",
        "    t = torch.arange(T_max).unsqueeze(0).expand(B, T_max)\n",
        "    mask = t < lengths.unsqueeze(1)\n",
        "    return x_pad, a_pad, y, v, lengths, mask\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Diagnostics helpers (reuse from your current code)\n",
        "# ============================================================\n",
        "def infer_cumulative_cols(x_cols: List[str]) -> Set[str]:\n",
        "    keys = [\"cumulative\", \"culmulative\", \"culvolume\", \"_auc\", \"auc\"]\n",
        "    out = set()\n",
        "    for c in x_cols:\n",
        "        cl = c.lower()\n",
        "        if any(k in cl for k in keys):\n",
        "            out.add(c)\n",
        "    return out\n",
        "\n",
        "def diff_within_surgery(df: pd.DataFrame, col: str) -> pd.Series:\n",
        "    d = df.groupby([PATIENT_ID_COL, CASE_ID_COL], sort=False)[col].diff()\n",
        "    d = d.fillna(df[col])\n",
        "    d = d.clip(lower=0.0)\n",
        "    return d.astype(np.float32)\n",
        "\n",
        "def detect_binary_cols(df: pd.DataFrame, cols: List[str]) -> Set[str]:\n",
        "    binary = set()\n",
        "    for c in cols:\n",
        "        if c not in df.columns:\n",
        "            continue\n",
        "        v = df[c].dropna()\n",
        "        if v.empty:\n",
        "            continue\n",
        "        uniq = np.unique(v.to_numpy(dtype=np.float32))\n",
        "        if len(uniq) <= 2 and set(uniq.tolist()).issubset({0.0, 1.0}):\n",
        "            binary.add(c)\n",
        "    return binary\n",
        "\n",
        "def list_negative_cols(df: pd.DataFrame, cols: List[str]) -> List[str]:\n",
        "    neg = []\n",
        "    for c in cols:\n",
        "        if c not in df.columns:\n",
        "            continue\n",
        "        try:\n",
        "            if float(df[c].min()) < 0:\n",
        "                neg.append(c)\n",
        "        except Exception:\n",
        "            pass\n",
        "    return sorted(neg)\n",
        "\n",
        "def alignment_diagnostics(intra_keys: Set[Tuple[str, str]], post_keys: Set[Tuple[str, str]], sample_n: int = 10) -> Dict[str, Any]:\n",
        "    inter = intra_keys & post_keys\n",
        "    only_in_intra = list(intra_keys - post_keys)\n",
        "    only_in_post  = list(post_keys - intra_keys)\n",
        "    return {\n",
        "        \"intra_unique_keys\": len(intra_keys),\n",
        "        \"post_unique_keys\": len(post_keys),\n",
        "        \"intersection_keys\": len(inter),\n",
        "        \"missing_in_post_y_count\": len(only_in_intra),\n",
        "        \"missing_in_intra_count\": len(only_in_post),\n",
        "        \"missing_in_post_y_sample\": only_in_intra[:sample_n],\n",
        "        \"missing_in_intra_sample\": only_in_post[:sample_n],\n",
        "    }\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Post_y loader + diagnostics\n",
        "# ============================================================\n",
        "def load_post_y_with_diagnostics(post_y_path: str, pid_width: Optional[int] = None) -> Tuple[Dict[Tuple[str, str], int], Dict[str, Any]]:\n",
        "    dfy = pd.read_csv(\n",
        "        post_y_path,\n",
        "        dtype={PATIENT_ID_COL: \"string\", CASE_ID_COL: \"string\"},\n",
        "        low_memory=False\n",
        "    )\n",
        "\n",
        "    dfy[PATIENT_ID_COL] = normalize_patient_id(dfy[PATIENT_ID_COL], width=pid_width)\n",
        "    dfy[CASE_ID_COL]    = normalize_case_id(dfy[CASE_ID_COL])\n",
        "\n",
        "    dfy[Y_LABEL_COL] = pd.to_numeric(dfy[Y_LABEL_COL], errors=\"coerce\")\n",
        "    dfy = dfy.dropna(subset=[Y_LABEL_COL])\n",
        "    dfy = dfy[dfy[Y_LABEL_COL].isin([0, 1])]\n",
        "\n",
        "    dup_counts = dfy.groupby([PATIENT_ID_COL, CASE_ID_COL]).size()\n",
        "    dup_keys = dup_counts[dup_counts > 1].index.tolist()\n",
        "\n",
        "    conflict_keys = []\n",
        "    if dup_keys:\n",
        "        nunique = dfy.groupby([PATIENT_ID_COL, CASE_ID_COL])[Y_LABEL_COL].nunique()\n",
        "        conflict_keys = nunique[nunique > 1].index.tolist()\n",
        "\n",
        "    dfy = dfy.drop_duplicates(subset=[PATIENT_ID_COL, CASE_ID_COL], keep=\"last\")\n",
        "\n",
        "    label_map: Dict[Tuple[str, str], int] = {}\n",
        "    for _, r in dfy.iterrows():\n",
        "        key = (str(r[PATIENT_ID_COL]), str(r[CASE_ID_COL]))\n",
        "        label_map[key] = int(r[Y_LABEL_COL])\n",
        "\n",
        "    diag = {\n",
        "        \"post_y_rows\": int(len(dfy)),\n",
        "        \"post_y_unique_keys\": int(len(label_map)),\n",
        "        \"post_y_duplicate_keys_count\": int(len(dup_keys)),\n",
        "        \"post_y_conflicting_label_keys_count\": int(len(conflict_keys)),\n",
        "        \"post_y_conflicting_label_sample\": conflict_keys[:10],\n",
        "    }\n",
        "    return label_map, diag\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Static v loaders\n",
        "#   - ethnicity: one-hot (train-fitted) + UNK bucket\n",
        "#   - ASA_class: default one-hot (1..5) + UNK, or ordinal\n",
        "# ============================================================\n",
        "def _safe_read_csv(path: str, encoding: Optional[str] = None, **kwargs) -> pd.DataFrame:\n",
        "    try:\n",
        "        return pd.read_csv(path, encoding=encoding, **kwargs)\n",
        "    except UnicodeDecodeError:\n",
        "        # fallback (common in csv exports)\n",
        "        return pd.read_csv(path, encoding=\"utf-8-sig\", **kwargs)\n",
        "\n",
        "def load_pre_v_df(pre_v_path: str, pid_width: Optional[int] = None) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    df = _safe_read_csv(\n",
        "        pre_v_path,\n",
        "        encoding=\"utf-8\",\n",
        "        dtype={PATIENT_ID_COL: \"string\", CASE_ID_COL: \"string\"},\n",
        "        low_memory=False\n",
        "    )\n",
        "\n",
        "    df[PATIENT_ID_COL] = normalize_patient_id(df[PATIENT_ID_COL], width=pid_width)\n",
        "    df[CASE_ID_COL]    = normalize_case_id(df[CASE_ID_COL])\n",
        "\n",
        "    missing = [c for c in PRE_V_COLS if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing columns in Pre_v.csv: {missing}\")\n",
        "\n",
        "    keep = [PATIENT_ID_COL, CASE_ID_COL] + PRE_V_COLS\n",
        "    df = df[keep].copy()\n",
        "\n",
        "    # gender: 男=1, 女=0\n",
        "    g = df[\"gender\"].astype(str).str.strip()\n",
        "    df[\"gender\"] = np.where(g == \"男\", 1.0, np.where(g == \"女\", 0.0, np.nan)).astype(np.float32)\n",
        "\n",
        "    # ethnicity: keep as string (encoded later)\n",
        "    df[\"ethnicity\"] = df[\"ethnicity\"].astype(str).str.strip().replace({\"nan\": \"\", \"None\": \"\", \"<NA>\": \"\"})\n",
        "\n",
        "    # numeric conversions for the rest (leave ethnicity)\n",
        "    numeric_cols = [c for c in PRE_V_COLS if c not in [\"gender\", \"ethnicity\"]]\n",
        "    for c in numeric_cols:\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    df[numeric_cols] = df[numeric_cols].astype(np.float32)\n",
        "\n",
        "    # duplicates diagnostics\n",
        "    dup_counts = df.groupby([PATIENT_ID_COL, CASE_ID_COL]).size()\n",
        "    dup_keys = dup_counts[dup_counts > 1].index.tolist()\n",
        "    df = df.drop_duplicates(subset=[PATIENT_ID_COL, CASE_ID_COL], keep=\"last\")\n",
        "\n",
        "    diag = {\n",
        "        \"pre_v_rows\": int(len(df)),\n",
        "        \"pre_v_unique_keys\": int(df[[PATIENT_ID_COL, CASE_ID_COL]].drop_duplicates().shape[0]),\n",
        "        \"pre_v_duplicate_keys_count\": int(len(dup_keys)),\n",
        "        \"pre_v_duplicate_keys_sample\": dup_keys[:10],\n",
        "    }\n",
        "    return df, diag\n",
        "\n",
        "def load_intra_v_df(intra_v_path: str, pid_width: Optional[int] = None) -> Tuple[pd.DataFrame, Dict[str, Any]]:\n",
        "    df = pd.read_csv(\n",
        "        intra_v_path,\n",
        "        dtype={PATIENT_ID_COL: \"string\", CASE_ID_COL: \"string\"},\n",
        "        low_memory=False\n",
        "    )\n",
        "    df[PATIENT_ID_COL] = normalize_patient_id(df[PATIENT_ID_COL], width=pid_width)\n",
        "    df[CASE_ID_COL]    = normalize_case_id(df[CASE_ID_COL])\n",
        "\n",
        "    missing = [c for c in INTRA_V_COLS if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing columns in Intra_v.csv: {missing}\")\n",
        "\n",
        "    keep = [PATIENT_ID_COL, CASE_ID_COL] + INTRA_V_COLS\n",
        "    df = df[keep].copy()\n",
        "\n",
        "    # is_emergency: 1 non-emergency, 2 emergency -> binary (emergency=1)\n",
        "    em = pd.to_numeric(df[\"is_emergency\"], errors=\"coerce\")\n",
        "    df[\"is_emergency\"] = np.where(em == 2, 1.0, np.where(em == 1, 0.0, np.nan)).astype(np.float32)\n",
        "\n",
        "    # ASA_class keep numeric for now (encoded later)\n",
        "    df[\"ASA_class\"] = pd.to_numeric(df[\"ASA_class\"], errors=\"coerce\")\n",
        "\n",
        "    # other ops -> numeric\n",
        "    op_cols = [c for c in INTRA_V_COLS if c not in [\"is_emergency\", \"ASA_class\"]]\n",
        "    for c in op_cols:\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "\n",
        "    df[op_cols] = df[op_cols].astype(np.float32)\n",
        "\n",
        "    dup_counts = df.groupby([PATIENT_ID_COL, CASE_ID_COL]).size()\n",
        "    dup_keys = dup_counts[dup_counts > 1].index.tolist()\n",
        "    df = df.drop_duplicates(subset=[PATIENT_ID_COL, CASE_ID_COL], keep=\"last\")\n",
        "\n",
        "    diag = {\n",
        "        \"intra_v_rows\": int(len(df)),\n",
        "        \"intra_v_unique_keys\": int(df[[PATIENT_ID_COL, CASE_ID_COL]].drop_duplicates().shape[0]),\n",
        "        \"intra_v_duplicate_keys_count\": int(len(dup_keys)),\n",
        "        \"intra_v_duplicate_keys_sample\": dup_keys[:10],\n",
        "    }\n",
        "    return df, diag\n",
        "\n",
        "\n",
        "def _index_by_key(df: pd.DataFrame) -> pd.DataFrame:\n",
        "    return df.set_index([PATIENT_ID_COL, CASE_ID_COL], drop=False)\n",
        "\n",
        "@dataclass\n",
        "class StaticEncoders:\n",
        "    ethnicity_categories: List[str]        # train-fitted, excludes \"\" (empty); UNK handled separately\n",
        "    asa_encoding: str                     # \"onehot\" or \"ordinal\"\n",
        "    asa_onehot_classes: List[int]         # [1,2,3,4,5] for onehot\n",
        "    v_col_names: List[str]                # final v dimension names (for debugging)\n",
        "\n",
        "def _fit_static_encoders(\n",
        "    train_keys: Sequence[Tuple[str, str]],\n",
        "    pre_df: pd.DataFrame,\n",
        "    intra_df: pd.DataFrame,\n",
        "    asa_encoding: str = \"onehot\",\n",
        ") -> StaticEncoders:\n",
        "    pre_i = _index_by_key(pre_df)\n",
        "    intra_i = _index_by_key(intra_df)\n",
        "\n",
        "    # ethnicity categories from TRAIN only (plus UNK)\n",
        "    eth_vals = []\n",
        "    for k in train_keys:\n",
        "        if k in pre_i.index:\n",
        "            v = str(pre_i.loc[k, \"ethnicity\"])\n",
        "            if v and v not in (\"nan\", \"None\", \"<NA>\"):\n",
        "                eth_vals.append(v)\n",
        "    eth_cats = sorted(list(set(eth_vals)))\n",
        "\n",
        "    # ASA classes: fixed 1..5 for onehot; ordinal uses numeric\n",
        "    asa_classes = [1, 2, 3, 4, 5]\n",
        "\n",
        "    # Build final v_col_names\n",
        "    # Pre numeric columns: all PRE_V_COLS except ethnicity (gender included here as numeric/binary)\n",
        "    pre_numeric = [c for c in PRE_V_COLS if c != \"ethnicity\"]\n",
        "    # We keep ethnicity as onehot in final vector:\n",
        "    eth_onehot_names = [f\"ethnicity__{c}\" for c in eth_cats] + [\"ethnicity__UNK\"]\n",
        "\n",
        "    # Intra numeric (excluding ASA if onehot)\n",
        "    intra_numeric = [c for c in INTRA_V_COLS if c != \"ASA_class\"]\n",
        "\n",
        "    if asa_encoding == \"onehot\":\n",
        "        asa_names = [f\"ASA_class__{c}\" for c in asa_classes] + [\"ASA_class__UNK\"]\n",
        "        v_cols = pre_numeric + eth_onehot_names + intra_numeric + asa_names\n",
        "    elif asa_encoding == \"ordinal\":\n",
        "        v_cols = pre_numeric + eth_onehot_names + intra_numeric + [\"ASA_class\"]\n",
        "    else:\n",
        "        raise ValueError(\"asa_encoding must be 'onehot' or 'ordinal'\")\n",
        "\n",
        "    return StaticEncoders(\n",
        "        ethnicity_categories=eth_cats,\n",
        "        asa_encoding=asa_encoding,\n",
        "        asa_onehot_classes=asa_classes,\n",
        "        v_col_names=v_cols,\n",
        "    )\n",
        "\n",
        "\n",
        "def _encode_onehot(value: str, categories: List[str]) -> np.ndarray:\n",
        "    out = np.zeros((len(categories) + 1,), dtype=np.float32)  # + UNK\n",
        "    if value in categories:\n",
        "        out[categories.index(value)] = 1.0\n",
        "    else:\n",
        "        out[-1] = 1.0\n",
        "    return out\n",
        "\n",
        "def _encode_asa(value: Any, enc: StaticEncoders) -> np.ndarray:\n",
        "    if enc.asa_encoding == \"ordinal\":\n",
        "        v = np.float32(np.nan if value is None else float(value))\n",
        "        if np.isnan(v):\n",
        "            v = np.float32(0.0)\n",
        "        return np.array([v], dtype=np.float32)\n",
        "\n",
        "    # onehot\n",
        "    out = np.zeros((len(enc.asa_onehot_classes) + 1,), dtype=np.float32)  # + UNK\n",
        "    try:\n",
        "        iv = int(float(value))\n",
        "    except Exception:\n",
        "        iv = None\n",
        "    if iv in enc.asa_onehot_classes:\n",
        "        out[enc.asa_onehot_classes.index(iv)] = 1.0\n",
        "    else:\n",
        "        out[-1] = 1.0\n",
        "    return out\n",
        "\n",
        "def build_v_for_key(\n",
        "    key: Tuple[str, str],\n",
        "    pre_i: pd.DataFrame,\n",
        "    intra_i: pd.DataFrame,\n",
        "    enc: StaticEncoders,\n",
        "    fill_missing_numeric: float = 0.0,\n",
        ") -> np.ndarray:\n",
        "    # Pre numeric (all except ethnicity)\n",
        "    pre_numeric_cols = [c for c in PRE_V_COLS if c != \"ethnicity\"]\n",
        "    if key in pre_i.index:\n",
        "        pre_row = pre_i.loc[key]\n",
        "        pre_num = np.array([pre_row.get(c, np.nan) for c in pre_numeric_cols], dtype=np.float32)\n",
        "        eth_val = str(pre_row.get(\"ethnicity\", \"\")).strip()\n",
        "    else:\n",
        "        pre_num = np.full((len(pre_numeric_cols),), np.nan, dtype=np.float32)\n",
        "        eth_val = \"\"\n",
        "\n",
        "    pre_num = np.nan_to_num(pre_num, nan=float(fill_missing_numeric)).astype(np.float32)\n",
        "    eth_oh = _encode_onehot(eth_val, enc.ethnicity_categories)\n",
        "\n",
        "    # Intra numeric (excluding ASA)\n",
        "    intra_numeric_cols = [c for c in INTRA_V_COLS if c != \"ASA_class\"]\n",
        "    if key in intra_i.index:\n",
        "        intra_row = intra_i.loc[key]\n",
        "        intra_num = np.array([intra_row.get(c, np.nan) for c in intra_numeric_cols], dtype=np.float32)\n",
        "        asa_val = intra_row.get(\"ASA_class\", np.nan)\n",
        "    else:\n",
        "        intra_num = np.full((len(intra_numeric_cols),), np.nan, dtype=np.float32)\n",
        "        asa_val = np.nan\n",
        "\n",
        "    intra_num = np.nan_to_num(intra_num, nan=float(fill_missing_numeric)).astype(np.float32)\n",
        "    asa_enc = _encode_asa(asa_val, enc)\n",
        "\n",
        "    return np.concatenate([pre_num, eth_oh, intra_num, asa_enc], axis=0).astype(np.float32)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Intra_xa: build sequences (now includes v placeholder)\n",
        "# ============================================================\n",
        "@dataclass\n",
        "class SurgerySequence:\n",
        "    x: np.ndarray\n",
        "    a: np.ndarray\n",
        "    y: int\n",
        "    v: np.ndarray\n",
        "    length: int\n",
        "    key: Tuple[str, str]\n",
        "\n",
        "class SurgeryDataset(Dataset):\n",
        "    def __init__(self, sequences: List[SurgerySequence]):\n",
        "        self.sequences = sequences\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.sequences)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        s = self.sequences[idx]\n",
        "        return (\n",
        "            torch.tensor(s.x, dtype=torch.float32),\n",
        "            torch.tensor(s.a, dtype=torch.long),\n",
        "            torch.tensor(float(s.y), dtype=torch.float32),\n",
        "            torch.tensor(s.v, dtype=torch.float32),\n",
        "            torch.tensor(s.length, dtype=torch.long),\n",
        "        )\n",
        "\n",
        "\n",
        "def build_sequences_from_intra_with_diagnostics(\n",
        "    intra_path: str,\n",
        "    label_map: Dict[Tuple[str, str], int],\n",
        "    pre_df: pd.DataFrame,\n",
        "    intra_v_df: pd.DataFrame,\n",
        "    static_enc: StaticEncoders,\n",
        "    use_incremental_for_cumulative_x: bool = True,\n",
        "    pid_width: Optional[int] = None,\n",
        "    drop_unlabeled: bool = True,\n",
        ") -> Tuple[List[SurgerySequence], Dict[str, Any], Dict[str, Any], Dict[str, Any]]:\n",
        "    df = pd.read_csv(\n",
        "        intra_path,\n",
        "        dtype={PATIENT_ID_COL: \"string\", CASE_ID_COL: \"string\"},\n",
        "        low_memory=False\n",
        "    )\n",
        "\n",
        "    df[PATIENT_ID_COL] = normalize_patient_id(df[PATIENT_ID_COL], width=pid_width)\n",
        "    df[CASE_ID_COL]    = normalize_case_id(df[CASE_ID_COL])\n",
        "\n",
        "    df[TIME_COL] = pd.to_numeric(df[TIME_COL], errors=\"coerce\")\n",
        "    df = df.dropna(subset=[TIME_COL])\n",
        "    df[TIME_COL] = df[TIME_COL].astype(int)\n",
        "\n",
        "    df[ACTION_CUM_COL] = pd.to_numeric(df[ACTION_CUM_COL], errors=\"coerce\").fillna(0.0).astype(np.float32)\n",
        "\n",
        "    missing_x = [c for c in X_COLS if c not in df.columns]\n",
        "    if missing_x:\n",
        "        raise ValueError(f\"Missing x cols in Intra_xa.csv: {missing_x}\")\n",
        "\n",
        "    for c in X_COLS:\n",
        "        df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
        "    df[X_COLS] = df[X_COLS].fillna(0.0).astype(np.float32)\n",
        "\n",
        "    df = df.sort_values([PATIENT_ID_COL, CASE_ID_COL, TIME_COL]).reset_index(drop=True)\n",
        "\n",
        "    # action incremental -> category\n",
        "    a_delta = df.groupby([PATIENT_ID_COL, CASE_ID_COL], sort=False)[ACTION_CUM_COL].diff()\n",
        "    a_delta = a_delta.fillna(df[ACTION_CUM_COL]).astype(np.float32).clip(lower=0.0)\n",
        "    df[\"_a_cat\"] = bin_incremental_action(a_delta.to_numpy())\n",
        "\n",
        "    # cumulative x option\n",
        "    cumulative_cols = infer_cumulative_cols(X_COLS)\n",
        "    negative_raw = list_negative_cols(df, X_COLS)\n",
        "\n",
        "    if use_incremental_for_cumulative_x and cumulative_cols:\n",
        "        for c in cumulative_cols:\n",
        "            df[c] = diff_within_surgery(df, c)\n",
        "\n",
        "    negative_post = list_negative_cols(df, X_COLS)\n",
        "    binary_x_cols = detect_binary_cols(df, X_COLS)\n",
        "\n",
        "    # static indices\n",
        "    pre_i = _index_by_key(pre_df)\n",
        "    intra_i = _index_by_key(intra_v_df)\n",
        "\n",
        "    # build sequences aligned\n",
        "    sequences: List[SurgerySequence] = []\n",
        "    grouped = df.groupby([PATIENT_ID_COL, CASE_ID_COL], sort=False)\n",
        "\n",
        "    intra_keys = set(grouped.groups.keys())\n",
        "    post_keys = set(label_map.keys())\n",
        "\n",
        "    kept = 0\n",
        "    dropped = 0\n",
        "    missing_pre = 0\n",
        "    missing_intra_v = 0\n",
        "\n",
        "    for key, g in grouped:\n",
        "        key = (str(key[0]), str(key[1]))\n",
        "\n",
        "        if key not in label_map:\n",
        "            if drop_unlabeled:\n",
        "                dropped += 1\n",
        "                continue\n",
        "            y = 0\n",
        "        else:\n",
        "            y = int(label_map[key])\n",
        "\n",
        "        if key not in pre_i.index:\n",
        "            missing_pre += 1\n",
        "        if key not in intra_i.index:\n",
        "            missing_intra_v += 1\n",
        "\n",
        "        g_sorted = g.sort_values(TIME_COL)\n",
        "        x = g_sorted[X_COLS].to_numpy(dtype=np.float32)\n",
        "        a = g_sorted[\"_a_cat\"].to_numpy(dtype=np.int64)\n",
        "        T = int(len(g_sorted))\n",
        "\n",
        "        v = build_v_for_key(key, pre_i, intra_i, static_enc, fill_missing_numeric=0.0)\n",
        "\n",
        "        sequences.append(SurgerySequence(x=x, a=a, y=y, v=v, length=T, key=key))\n",
        "        kept += 1\n",
        "\n",
        "    intra_diag = {\n",
        "        \"intra_rows\": int(len(df)),\n",
        "        \"intra_unique_keys\": int(len(intra_keys)),\n",
        "        \"total_surgeries_in_intra\": int(len(intra_keys)),\n",
        "        \"kept_aligned\": int(kept),\n",
        "        \"dropped_unlabeled\": int(dropped),\n",
        "        \"cumulative_like_cols\": sorted(list(cumulative_cols)),\n",
        "        \"binary_cols_detected\": sorted(list(binary_x_cols)),\n",
        "        \"negative_x_cols_raw\": negative_raw,\n",
        "        \"negative_x_cols_post\": negative_post,\n",
        "    }\n",
        "\n",
        "    align_diag = alignment_diagnostics(\n",
        "        intra_keys=set(map(lambda t: (str(t[0]), str(t[1])), intra_keys)),\n",
        "        post_keys=set(map(lambda t: (str(t[0]), str(t[1])), post_keys)),\n",
        "        sample_n=10,\n",
        "    )\n",
        "\n",
        "    static_diag = {\n",
        "        \"static_dim_v\": int(len(static_enc.v_col_names)),\n",
        "        \"missing_pre_v_count_in_intra\": int(missing_pre),\n",
        "        \"missing_intra_v_count_in_intra\": int(missing_intra_v),\n",
        "        \"pre_v_unique_keys\": int(pre_df[[PATIENT_ID_COL, CASE_ID_COL]].drop_duplicates().shape[0]),\n",
        "        \"intra_v_unique_keys\": int(intra_v_df[[PATIENT_ID_COL, CASE_ID_COL]].drop_duplicates().shape[0]),\n",
        "    }\n",
        "\n",
        "    return sequences, intra_diag, align_diag, static_diag\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Normalization plans\n",
        "#   - same rule as x:\n",
        "#       binary/onehot -> none\n",
        "#       nonneg -> scale-only\n",
        "#       else -> zscore\n",
        "# ============================================================\n",
        "@dataclass\n",
        "class NormalizationPlan:\n",
        "    methods: List[str]       # \"none\" | \"scale\" | \"zscore\"\n",
        "    mean: np.ndarray\n",
        "    std: np.ndarray\n",
        "    scale_cols: List[str]\n",
        "    zscore_cols: List[str]\n",
        "    none_cols: List[str]\n",
        "\n",
        "def make_normalization_plan_from_matrix(\n",
        "    X_train: np.ndarray,\n",
        "    col_names: List[str],\n",
        "    binary_cols: Set[str],\n",
        "    force_scale_cols: Set[str] | None = None,\n",
        "    scale_nonneg_only: bool = True,\n",
        ") -> NormalizationPlan:\n",
        "    if force_scale_cols is None:\n",
        "        force_scale_cols = set()\n",
        "\n",
        "    mean = X_train.mean(axis=0, dtype=np.float64).astype(np.float32)\n",
        "    std  = X_train.std(axis=0, dtype=np.float64).astype(np.float32)\n",
        "    std  = np.where(std < 1e-8, 1.0, std).astype(np.float32)\n",
        "    mins = X_train.min(axis=0)\n",
        "\n",
        "    methods: List[str] = []\n",
        "    scale_cols, zscore_cols, none_cols = [], [], []\n",
        "\n",
        "    for j, c in enumerate(col_names):\n",
        "        if c in binary_cols:\n",
        "            methods.append(\"none\")\n",
        "            none_cols.append(c)\n",
        "            continue\n",
        "\n",
        "        if c in force_scale_cols:\n",
        "            methods.append(\"scale\")\n",
        "            scale_cols.append(c)\n",
        "            continue\n",
        "\n",
        "        if scale_nonneg_only and mins[j] >= 0.0:\n",
        "            methods.append(\"scale\")\n",
        "            scale_cols.append(c)\n",
        "        else:\n",
        "            methods.append(\"zscore\")\n",
        "            zscore_cols.append(c)\n",
        "\n",
        "    return NormalizationPlan(\n",
        "        methods=methods, mean=mean, std=std,\n",
        "        scale_cols=scale_cols, zscore_cols=zscore_cols, none_cols=none_cols\n",
        "    )\n",
        "\n",
        "def apply_plan_inplace_matrix(X: np.ndarray, plan: NormalizationPlan) -> np.ndarray:\n",
        "    X = X.astype(np.float32, copy=True)\n",
        "    for j, method in enumerate(plan.methods):\n",
        "        if method == \"none\":\n",
        "            continue\n",
        "        elif method == \"scale\":\n",
        "            X[:, j] = X[:, j] / plan.std[j]\n",
        "        elif method == \"zscore\":\n",
        "            X[:, j] = (X[:, j] - plan.mean[j]) / plan.std[j]\n",
        "        else:\n",
        "            raise ValueError(method)\n",
        "    return X\n",
        "\n",
        "def apply_normalization_inplace_sequences_x(sequences: List[SurgerySequence], plan: NormalizationPlan) -> None:\n",
        "    for s in sequences:\n",
        "        # s.x is (T, dim_x)\n",
        "        X = s.x.astype(np.float32, copy=True)\n",
        "        for j, method in enumerate(plan.methods):\n",
        "            if method == \"none\":\n",
        "                continue\n",
        "            elif method == \"scale\":\n",
        "                X[:, j] = X[:, j] / plan.std[j]\n",
        "            elif method == \"zscore\":\n",
        "                X[:, j] = (X[:, j] - plan.mean[j]) / plan.std[j]\n",
        "            else:\n",
        "                raise ValueError(method)\n",
        "        s.x = X\n",
        "\n",
        "def apply_normalization_inplace_sequences_v(sequences: List[SurgerySequence], plan: NormalizationPlan) -> None:\n",
        "    for s in sequences:\n",
        "        s.v = apply_plan_inplace_matrix(s.v.reshape(1, -1), plan).reshape(-1).astype(np.float32)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Main entry\n",
        "# ============================================================\n",
        "def create_loaders(\n",
        "    use_incremental_for_cumulative_x: bool = True,\n",
        "    pid_width: Optional[int] = None,\n",
        "    train_frac: float = 0.8,\n",
        "    seed: int = 0,\n",
        "    batch_size_train: int = 32,\n",
        "    batch_size_test: int = 64,\n",
        "    normalize_x: bool = True,\n",
        "    normalize_v: bool = True,\n",
        "    scale_nonneg_only: bool = True,\n",
        "    plot_length_hist: bool = True,\n",
        "    asa_encoding: str = \"onehot\",   # \"onehot\" (recommended) or \"ordinal\"\n",
        "):\n",
        "    # 1) y labels\n",
        "    label_map, post_diag = load_post_y_with_diagnostics(POST_Y_PATH, pid_width=pid_width)\n",
        "\n",
        "    # 2) static dfs\n",
        "    pre_df, pre_diag = load_pre_v_df(PRE_V_PATH, pid_width=pid_width)\n",
        "    intra_v_df, intra_v_diag = load_intra_v_df(INTRA_V_PATH, pid_width=pid_width)\n",
        "\n",
        "    # 3) We will split later, but we need encoders fitted on TRAIN keys.\n",
        "    #    So first: load Intra_xa keys only (cheap pass) to build candidate key list.\n",
        "    #    We will actually build sequences AFTER fitting encoders, but fitting needs train keys.\n",
        "    #    To avoid extra passes, we read Intra_xa minimal columns to get key list.\n",
        "    tmp = pd.read_csv(\n",
        "        INTRA_XA_PATH,\n",
        "        dtype={PATIENT_ID_COL: \"string\", CASE_ID_COL: \"string\"},\n",
        "        usecols=[PATIENT_ID_COL, CASE_ID_COL, TIME_COL],\n",
        "        low_memory=False\n",
        "    )\n",
        "    tmp[PATIENT_ID_COL] = normalize_patient_id(tmp[PATIENT_ID_COL], width=pid_width)\n",
        "    tmp[CASE_ID_COL]    = normalize_case_id(tmp[CASE_ID_COL])\n",
        "    intra_keys_all = tmp[[PATIENT_ID_COL, CASE_ID_COL]].drop_duplicates()\n",
        "    all_keys = [(str(r[PATIENT_ID_COL]), str(r[CASE_ID_COL])) for _, r in intra_keys_all.iterrows()]\n",
        "\n",
        "    # For train/test split reproducibility\n",
        "    rng = np.random.RandomState(int(seed))\n",
        "    perm = rng.permutation(len(all_keys))\n",
        "    n_train_keys = int(round(train_frac * len(all_keys)))\n",
        "    n_train_keys = max(1, min(n_train_keys, len(all_keys) - 1))\n",
        "    train_keys = [all_keys[i] for i in perm[:n_train_keys]]\n",
        "    # Fit encoders on TRAIN keys only\n",
        "    static_enc = _fit_static_encoders(train_keys, pre_df, intra_v_df, asa_encoding=asa_encoding)\n",
        "\n",
        "    # 4) build sequences (x,a,y,v) + diagnostics\n",
        "    sequences, intra_diag, align_diag, static_diag = build_sequences_from_intra_with_diagnostics(\n",
        "        INTRA_XA_PATH,\n",
        "        label_map,\n",
        "        pre_df=pre_df,\n",
        "        intra_v_df=intra_v_df,\n",
        "        static_enc=static_enc,\n",
        "        use_incremental_for_cumulative_x=use_incremental_for_cumulative_x,\n",
        "        pid_width=pid_width,\n",
        "        drop_unlabeled=True\n",
        "    )\n",
        "\n",
        "    print(\"\\n=== Alignment diagnostics (matching by (patient_id, case_id) as strings) ===\")\n",
        "    print(align_diag)\n",
        "\n",
        "    print(\"\\n=== Post_y diagnostics ===\")\n",
        "    print(post_diag)\n",
        "\n",
        "    print(\"\\n=== Pre_v diagnostics ===\")\n",
        "    print(pre_diag)\n",
        "\n",
        "    print(\"\\n=== Intra_v diagnostics ===\")\n",
        "    print(intra_v_diag)\n",
        "\n",
        "    print(\"\\n=== Intra_xa diagnostics ===\")\n",
        "    print(intra_diag)\n",
        "\n",
        "    print(\"\\n=== Static v diagnostics ===\")\n",
        "    print(static_diag)\n",
        "    print(f\"ASA encoding: {asa_encoding} | ethnicity categories (train-fitted): {len(static_enc.ethnicity_categories)}\")\n",
        "    print(f\"dim_v = {static_diag['static_dim_v']}\")\n",
        "\n",
        "    # 5) Histogram of sequence lengths\n",
        "    if plot_length_hist:\n",
        "        lengths_arr = np.array([s.length for s in sequences], dtype=int)\n",
        "        plt.figure()\n",
        "        plt.hist(lengths_arr, bins=30)\n",
        "        plt.title(\"Histogram of sequence lengths\")\n",
        "        plt.xlabel(\"Sequence length (time steps)\")\n",
        "        plt.ylabel(\"Count\")\n",
        "        plt.show()\n",
        "\n",
        "    # 6) Build dataset + split (now on final sequences list)\n",
        "    dataset = SurgeryDataset(sequences)\n",
        "    n_total = len(dataset)\n",
        "    n_train = int(round(train_frac * n_total))\n",
        "    n_train = max(1, min(n_train, n_total - 1))\n",
        "    n_test = n_total - n_train\n",
        "\n",
        "    g = torch.Generator().manual_seed(int(seed))\n",
        "    train_subset, test_subset = random_split(dataset, [n_train, n_test], generator=g)\n",
        "\n",
        "    # 7) Normalize x (train-split)\n",
        "    if normalize_x:\n",
        "        binary_x_cols = set(intra_diag[\"binary_cols_detected\"])\n",
        "        cumulative_cols = set(intra_diag[\"cumulative_like_cols\"])\n",
        "\n",
        "        train_seqs = [dataset.sequences[i] for i in train_subset.indices]\n",
        "        X_train = np.concatenate([s.x for s in train_seqs], axis=0)  # (sumT, dim_x)\n",
        "\n",
        "        # If you keep cumulative representation, force cumulative-like cols to scale-only\n",
        "        force_scale = set()\n",
        "        if not use_incremental_for_cumulative_x:\n",
        "            force_scale = cumulative_cols\n",
        "\n",
        "        x_plan = make_normalization_plan_from_matrix(\n",
        "            X_train=X_train,\n",
        "            col_names=X_COLS,\n",
        "            binary_cols=binary_x_cols,\n",
        "            force_scale_cols=force_scale,\n",
        "            scale_nonneg_only=scale_nonneg_only\n",
        "        )\n",
        "        apply_normalization_inplace_sequences_x(dataset.sequences, x_plan)\n",
        "\n",
        "        print(\"\\n=== X normalization lists (based on TRAIN split stats) ===\")\n",
        "        print(\"x <- x / std (scale-only):\")\n",
        "        print(x_plan.scale_cols)\n",
        "        print(\"\\nx <- (x - mean) / std (z-score):\")\n",
        "        print(x_plan.zscore_cols)\n",
        "        print(\"\\n(no normalization):\")\n",
        "        print(x_plan.none_cols)\n",
        "\n",
        "        print(\"\\n=== Negative-value x columns ===\")\n",
        "        print(\"Negative values in RAW x (before cumulative->incremental):\")\n",
        "        print(intra_diag[\"negative_x_cols_raw\"])\n",
        "        print(\"\\nNegative values AFTER your chosen representation:\")\n",
        "        print(intra_diag[\"negative_x_cols_post\"])\n",
        "\n",
        "    # 8) Normalize v (train-split)\n",
        "    if normalize_v:\n",
        "        train_seqs = [dataset.sequences[i] for i in train_subset.indices]\n",
        "        V_train = np.stack([s.v for s in train_seqs], axis=0)  # (n_train, dim_v)\n",
        "\n",
        "        # Detect binary / onehot columns in v by name:\n",
        "        # - onehot columns contain \"__\"\n",
        "        # - also treat known binary-like source cols as binary by name\n",
        "        v_cols = static_enc.v_col_names\n",
        "        v_binary_cols = set([c for c in v_cols if \"__\" in c])  # onehot -> none\n",
        "\n",
        "        # Also mark obvious binary flags by source name:\n",
        "        likely_binary_prefixes = {\n",
        "            \"gender\",\n",
        "            \"is_emergency\",\n",
        "            \"CABG_oper\",\n",
        "            \"CardiacTumor_oper\",\n",
        "            \"HeartTransplant_oper\",\n",
        "            \"aortic_oper\",\n",
        "            \"congenital_oper\",\n",
        "            \"valve_oper\",\n",
        "        }\n",
        "        for c in v_cols:\n",
        "            base = c.split(\"__\")[0]  # ethnicity__X -> ethnicity\n",
        "            if base in likely_binary_prefixes:\n",
        "                v_binary_cols.add(c)\n",
        "\n",
        "        # Additionally, we can auto-detect binary among non-onehot columns using V_train values:\n",
        "        # (keep this conservative; onehot already covered)\n",
        "        for j, c in enumerate(v_cols):\n",
        "            if c in v_binary_cols:\n",
        "                continue\n",
        "            uniq = np.unique(V_train[:, j].astype(np.float32))\n",
        "            if len(uniq) <= 2 and set(uniq.tolist()).issubset({0.0, 1.0}):\n",
        "                v_binary_cols.add(c)\n",
        "\n",
        "        v_plan = make_normalization_plan_from_matrix(\n",
        "            X_train=V_train,\n",
        "            col_names=v_cols,\n",
        "            binary_cols=v_binary_cols,\n",
        "            force_scale_cols=set(),  # no special forcing beyond binary/onehot\n",
        "            scale_nonneg_only=scale_nonneg_only\n",
        "        )\n",
        "        apply_normalization_inplace_sequences_v(dataset.sequences, v_plan)\n",
        "\n",
        "        # v negatives diagnostics\n",
        "        neg_v_cols = []\n",
        "        for j, c in enumerate(v_cols):\n",
        "            if np.min(V_train[:, j]) < 0:\n",
        "                neg_v_cols.append(c)\n",
        "\n",
        "        print(\"\\n=== V normalization lists (based on TRAIN split stats) ===\")\n",
        "        print(\"v <- v / std (scale-only):\")\n",
        "        print(v_plan.scale_cols)\n",
        "        print(\"\\nv <- (v - mean) / std (z-score):\")\n",
        "        print(v_plan.zscore_cols)\n",
        "        print(\"\\n(no normalization):\")\n",
        "        print(v_plan.none_cols)\n",
        "\n",
        "        print(\"\\n=== Negative-value v columns (on TRAIN matrix before normalization) ===\")\n",
        "        print(neg_v_cols)\n",
        "\n",
        "    # 9) Loaders\n",
        "    train_loader = DataLoader(\n",
        "        train_subset,\n",
        "        batch_size=batch_size_train,\n",
        "        shuffle=True,\n",
        "        collate_fn=lambda b: pad_collate_varlen(b, pad_action=0),\n",
        "        drop_last=False\n",
        "    )\n",
        "    test_loader = DataLoader(\n",
        "        test_subset,\n",
        "        batch_size=batch_size_test,\n",
        "        shuffle=False,\n",
        "        collate_fn=lambda b: pad_collate_varlen(b, pad_action=0),\n",
        "        drop_last=False\n",
        "    )\n",
        "\n",
        "    # 10) Sanity check shapes\n",
        "    x_pad, a_pad, y, v, lengths, mask = next(iter(train_loader))\n",
        "    print(\"\\n=== Batch shapes ===\")\n",
        "    print(\"x_pad:\", tuple(x_pad.shape),\n",
        "          \"a_pad:\", tuple(a_pad.shape),\n",
        "          \"y:\", tuple(y.shape),\n",
        "          \"v:\", tuple(v.shape))\n",
        "    print(\"lengths:\", tuple(lengths.shape), \"mask:\", tuple(mask.shape))\n",
        "\n",
        "    return train_loader, test_loader, {\n",
        "        \"align_diag\": align_diag,\n",
        "        \"post_diag\": post_diag,\n",
        "        \"pre_diag\": pre_diag,\n",
        "        \"intra_v_diag\": intra_v_diag,\n",
        "        \"intra_diag\": intra_diag,\n",
        "        \"static_diag\": static_diag,\n",
        "        \"static_enc\": {\n",
        "            \"asa_encoding\": static_enc.asa_encoding,\n",
        "            \"ethnicity_categories\": static_enc.ethnicity_categories,\n",
        "            \"v_col_names\": static_enc.v_col_names,\n",
        "        },\n",
        "        \"n_total\": n_total,\n",
        "        \"n_train\": n_train,\n",
        "        \"n_test\": n_test,\n",
        "    }\n",
        "\n",
        "\n",
        "# Example:\n",
        "if __name__ == \"__main__\":\n",
        "    train_loader, test_loader, report = create_loaders(\n",
        "        use_incremental_for_cumulative_x=True,\n",
        "        pid_width=None,\n",
        "        normalize_x=True,\n",
        "        normalize_v=True,\n",
        "        scale_nonneg_only=True,\n",
        "        plot_length_hist=True,\n",
        "        asa_encoding=\"onehot\",  # or \"ordinal\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fagj1xmeEIEA"
      },
      "source": [
        "# Transfusion-ITE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wY7HhqKeEubm",
        "outputId": "d1804b03-ec7a-4585-d587-36c37f847de6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyro-ppl\n",
            "  Downloading pyro_ppl-1.9.1-py3-none-any.whl.metadata (7.8 kB)\n",
            "Requirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.12/dist-packages (from pyro-ppl) (2.0.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from pyro-ppl) (3.4.0)\n",
            "Collecting pyro-api>=0.1.1 (from pyro-ppl)\n",
            "  Downloading pyro_api-0.1.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.12/dist-packages (from pyro-ppl) (2.9.0+cpu)\n",
            "Requirement already satisfied: tqdm>=4.36 in /usr/local/lib/python3.12/dist-packages (from pyro-ppl) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (3.20.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch>=2.0->pyro-ppl) (2025.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=2.0->pyro-ppl) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=2.0->pyro-ppl) (3.0.3)\n",
            "Downloading pyro_ppl-1.9.1-py3-none-any.whl (755 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m756.0/756.0 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyro_api-0.1.2-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: pyro-api, pyro-ppl\n",
            "Successfully installed pyro-api-0.1.2 pyro-ppl-1.9.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyro-ppl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "c2Fepi3MEw9r"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import files\n",
        "import math\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.gridspec as gridspec\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset, random_split\n",
        "import torch.distributions as td\n",
        "from torch.distributions import constraints\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import pyro\n",
        "import pyro.poutine as poutine\n",
        "import pyro.distributions as dist\n",
        "from pyro.distributions import TransformedDistribution\n",
        "from pyro.distributions.torch_distribution import TorchDistribution\n",
        "from pyro.distributions.transforms import affine_autoregressive\n",
        "from pyro.infer import (\n",
        "    SVI,\n",
        "    JitTrace_ELBO,\n",
        "    Trace_ELBO,\n",
        "    TraceEnum_ELBO,\n",
        "    TraceTMC_ELBO,\n",
        "    config_enumerate,\n",
        ")\n",
        "from pyro.optim import (\n",
        "    Adam,\n",
        "    ClippedAdam,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "JNvMsGSuETE7",
        "outputId": "2aa732d4-3484-4cb5-b4bb-6d60b68451b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Alignment diagnostics (matching by (patient_id, case_id) as strings) ===\n",
            "{'intra_unique_keys': 26904, 'post_unique_keys': 26955, 'intersection_keys': 26904, 'missing_in_post_y_count': 0, 'missing_in_intra_count': 51, 'missing_in_post_y_sample': [], 'missing_in_intra_sample': [('BAH0014070', 'BLH0013253'), ('BAH0006661', 'BLH0015509'), ('BAH0017194', 'BLH0016503'), ('BAH0018845', 'BLH0018407'), ('BAH0008319', 'BLH0006486'), ('BAH0012894', 'BLH0011512'), ('BAH0009868', 'BLH0015342'), ('BAH0020671', 'BLH0020631'), ('BAH0016512', 'BLH0015639'), ('BAH0016219', 'BLH0015300')]}\n",
            "\n",
            "=== Post_y diagnostics ===\n",
            "{'post_y_rows': 26955, 'post_y_unique_keys': 26955, 'post_y_duplicate_keys_count': 0, 'post_y_conflicting_label_keys_count': 0, 'post_y_conflicting_label_sample': []}\n",
            "\n",
            "=== Pre_v diagnostics ===\n",
            "{'pre_v_rows': 26955, 'pre_v_unique_keys': 26955, 'pre_v_duplicate_keys_count': 0, 'pre_v_duplicate_keys_sample': []}\n",
            "\n",
            "=== Intra_v diagnostics ===\n",
            "{'intra_v_rows': 26955, 'intra_v_unique_keys': 26955, 'intra_v_duplicate_keys_count': 0, 'intra_v_duplicate_keys_sample': []}\n",
            "\n",
            "=== Intra_xa diagnostics ===\n",
            "{'intra_rows': 202240, 'intra_unique_keys': 26904, 'total_surgeries_in_intra': 26904, 'kept_aligned': 26904, 'dropped_unlabeled': 0, 'cumulative_like_cols': ['CPBresidualBlood_culVolume', 'intraoper_Autoblood_culVolume', 'intraoper_GCs_culmulative_value', 'intraoper_Plasma_culVolume', 'intraoper_Platelets_culVolume', 'intraoper_colloid_culVolume', 'intraoper_cumulative_HRV', 'intraoper_cumulative_MAP_auc', 'intraoper_cumulative_hypothermia_auc', 'intraoper_cumulative_hypoxic_auc'], 'binary_cols_detected': ['intraoper_Esmolol_used', 'intraoper_Inhalation_anesthetics', 'intraoper_Ketamine_value', 'intraoper_Propofol_used', 'intraoper_Terlipressin_used', 'isultrafilter_used'], 'negative_x_cols_raw': [], 'negative_x_cols_post': []}\n",
            "\n",
            "=== Static v diagnostics ===\n",
            "{'static_dim_v': 91, 'missing_pre_v_count_in_intra': 0, 'missing_intra_v_count_in_intra': 0, 'pre_v_unique_keys': 26955, 'intra_v_unique_keys': 26955}\n",
            "ASA encoding: onehot | ethnicity categories (train-fitted): 3\n",
            "dim_v = 91\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== X normalization lists (based on TRAIN split stats) ===\n",
            "x <- x / std (scale-only):\n",
            "['intraoper_cumulative_HRV', 'intraoper_cumulative_hypothermia_auc', 'intraoper_cumulative_hypoxic_auc', 'intraoper_cumulative_MAP_auc', 'intraoper_FIO2_ArtBGA', 'intraoper_Oxygenation_index', 'intraoper_tHb_value', 'intraoper_BE_value', 'intraoper_cHCO3_value', 'intraoper_CVP_value', 'intraoper_Ca_value', 'intraoper_Na_value', 'intraoper_K_value', 'intraoper_Glu_value', 'intraoper_Lac_value', 'intraoper_PH_value', 'intraoper_Norepinephrine_max', 'intraoper_Dopamine_max', 'intraoper_Dobutamine_max', 'intraoper_Isoproterenol_max', 'intraoper_Adrenaline_max', 'intraoper_Milrinone_max', 'intraoper_Baquting_value', 'intraoper_Tranexamic_Acid_value', 'intraoper_Heparin_value', 'intraoper_protamine_value', 'intraoper_opioids_MME_value', 'intraoper_GCs_culmulative_value', 'intraoper_Platelets_culVolume', 'intraoper_Plasma_culVolume', 'intraoper_Autoblood_culVolume', 'CPBresidualBlood_culVolume', 'intraoper_colloid_culVolume']\n",
            "\n",
            "x <- (x - mean) / std (z-score):\n",
            "[]\n",
            "\n",
            "(no normalization):\n",
            "['intraoper_Terlipressin_used', 'intraoper_Esmolol_used', 'intraoper_Ketamine_value', 'intraoper_Inhalation_anesthetics', 'intraoper_Propofol_used', 'isultrafilter_used']\n",
            "\n",
            "=== Negative-value x columns ===\n",
            "Negative values in RAW x (before cumulative->incremental):\n",
            "[]\n",
            "\n",
            "Negative values AFTER your chosen representation:\n",
            "[]\n",
            "\n",
            "=== V normalization lists (based on TRAIN split stats) ===\n",
            "v <- v / std (scale-only):\n",
            "['adm_age', 'BMI', 'preoper_DBP', 'preoper_Heart_Rate', 'preoper_Pulse', 'preoper_SBP', 'preoper_Temperature', 'preoper_Respiratory_Rate', 'd_NYHA_Level', 'd_arrhy_avb', 'd_hyper_hypo_thyroidism', 'preoperLab_NT_proBNP', 'preoperLab_CRP', 'preoperLab_Neutrophil_Percentage', 'preoperLab_eGFR', 'preoperLab_PT', 'preoperLab_TBIL', 'preoperLab_APTT', 'preoperLab_WBC_Count', 'preoperLab_ALB', 'preoperLab_RBC_Count', 'preoperLab_TnT', 'preoperLab_Cystatin_C', 'preoperLab_Platelet_Count', 'preoperLab_BUN', 'preoperLab_ESR', 'preoperLab_Hemoglobin', 'preoperLab_Glucose', 'preoper_LVEF']\n",
            "\n",
            "v <- (v - mean) / std (z-score):\n",
            "[]\n",
            "\n",
            "(no normalization):\n",
            "['gender', 'cardiac_surgery_history_adm', 'renal_surgery_history_adm', 'allergy_history_adm', 'drinking_history_adm', 'smoking_history_adm', 'transfusion_history_adm', 'd_Valve_Dis', 'd_Rheumatic_HD', 'd_Congenital_HD', 'd_Aortic_related_dis', 'd_Coronary_HD', 'd_Cardiac_Tumor', 'd_Cardiomyopathy', 'd_Infect_Endocarditis', 'd_Pericardial_Dis', 'd_Liver_Disease', 'd_CKD_Status', 'd_Cerebrovascular_Events', 'd_AF_af_Arrhythmia', 'd_arrhy_cp_icd_crt', 'd_auto_immune', 'd_copd', 'd_dm', 'd_lipn', 'd_pvd', 'd_sepsis', 'd_htn', 'd_pulmonary_hypertension', 'preoperDrug_Glucocorticoid_Usage', 'preoperDrug_Amphotericin_Aminoglycoside_Combined_Usage', 'preoperDrug_ACEIARB_Usage', 'preoperDrug_Diuretics_Usage', 'preoperDrug_NSAIDs_Usage', 'preoperDrug_Norepinephrine', 'preoperDrug_vasopressinUSE', 'preoperDrug_Dopamine', 'preoperDrug_Nitroprusside', 'preoperDrug_Dobutamine', 'preoperDrug_Isoproterenol', 'preoperDrug_Epinephrine', 'preoperDrug_Nitroglycerin', 'preoperDrug_Metaraminol', 'preoperDrug_Contrast_Exposure', 'pre_aki', 'ethnicity__1.0', 'ethnicity__2.0', 'ethnicity__3.0', 'ethnicity__UNK', 'is_emergency', 'CABG_oper', 'CardiacTumor_oper', 'HeartTransplant_oper', 'aortic_oper', 'congenital_oper', 'valve_oper', 'ASA_class__1', 'ASA_class__2', 'ASA_class__3', 'ASA_class__4', 'ASA_class__5', 'ASA_class__UNK']\n",
            "\n",
            "=== Negative-value v columns (on TRAIN matrix before normalization) ===\n",
            "[]\n",
            "\n",
            "=== Batch shapes ===\n",
            "x_pad: (32, 13, 39) a_pad: (32, 13) y: (32, 1) v: (32, 91)\n",
            "lengths: (32,) mask: (32, 13)\n",
            "[info] steps/epoch=673, total_steps=67300, kl_start_epoch=2, start_step=1346, ramp_steps=32977, ramp_end_step=34323\n",
            "epoch |  train loss/t | test@1.0 loss/t |  beta || distill KL/t (q_phi || q_tilde)\n",
            "----- | ------------ | -------------- | ----- || ---------------------------\n",
            "    1 |     208.7158 |       297.9428 | 0.000 ||                      6.6024\n",
            "    2 |     201.9442 |       292.7981 | 0.000 ||                     13.8850\n",
            "    3 |      75.8507 |       178.2378 | 0.020 ||                     10.0211\n",
            "    4 |      29.5450 |        48.6190 | 0.041 ||                      9.4571\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-892643238.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    879\u001b[0m \u001b[0;31m# If running as a script, this works; in notebooks, just call main([])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-892643238.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m    863\u001b[0m     )\n\u001b[1;32m    864\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m     train_svi(\n\u001b[0m\u001b[1;32m    866\u001b[0m         \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-892643238.py\u001b[0m in \u001b[0;36mtrain_svi\u001b[0;34m(train_loader, model, test_loader, epochs, lr, kl_start_epoch, seed, log_every, num_particles, distill_weight)\u001b[0m\n\u001b[1;32m    649\u001b[0m             \u001b[0mstep\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    650\u001b[0m             \u001b[0mbeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manneal_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 651\u001b[0;31m             \u001b[0msvi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mannealing_factor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistill_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdistill_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyro/infer/svi.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;31m# actually perform gradient steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;31m# torch.optim objects gets instantiated for any params that haven't been seen yet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;31m# zero gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyro/optim/optim.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, params, *args, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim_objs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 155\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim_objs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    156\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    515\u001b[0m                             )\n\u001b[1;32m    516\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 517\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    518\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyro/optim/clipped_adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;31m# Decay the first and second moment running average coefficient\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mexp_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m                 \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m                 \u001b[0mdenom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexp_avg_sq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"eps\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import math\n",
        "import random\n",
        "from typing import Any, Dict, List, Optional, Sequence, Tuple\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.distributions as td\n",
        "from torch.utils.data import DataLoader, Dataset, random_split\n",
        "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "import pyro\n",
        "import pyro.distributions as dist\n",
        "import pyro.poutine as poutine\n",
        "from pyro.infer import SVI, Trace_ELBO\n",
        "from pyro.optim import ClippedAdam\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Utils\n",
        "# ============================================================\n",
        "\n",
        "def set_all_seeds(seed: int) -> None:\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "def reverse_padded_sequence(seq: torch.Tensor, lengths: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Reverse each sequence up to its length, keep padding.\n",
        "    seq: (B, T, D)\n",
        "    lengths: (B,)\n",
        "    \"\"\"\n",
        "    B, T, D = seq.shape\n",
        "    out = seq.clone()\n",
        "    for i in range(B):\n",
        "        L = int(lengths[i].item())\n",
        "        if L > 0:\n",
        "            out[i, :L, :] = torch.flip(seq[i, :L, :], dims=[0])\n",
        "        if L < T:\n",
        "            out[i, L:, :] = 0.0\n",
        "    return out\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Neural components\n",
        "# ============================================================\n",
        "\n",
        "class MLPEmbed(nn.Module):\n",
        "    \"\"\"Embed static covariates v -> v_emb.\"\"\"\n",
        "    def __init__(self, in_dim: int, out_dim: int, hidden_dim: int = 128, dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, hidden_dim),\n",
        "            nn.Tanh(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, out_dim),\n",
        "            nn.Tanh(),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "class MLPNormal(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, hidden_dim, min_scale=0.1, max_scale=0.8):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.loc = nn.Linear(hidden_dim, out_dim)\n",
        "        self.scale = nn.Linear(hidden_dim, out_dim)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.softplus = nn.Softplus()\n",
        "        self.min_scale = float(min_scale)\n",
        "        self.max_scale = float(max_scale)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.tanh(self.fc1(x))\n",
        "        h = self.tanh(self.fc2(h))\n",
        "        loc = self.loc(h)\n",
        "        scale = (self.softplus(self.scale(h)) + self.min_scale).clamp(max=self.max_scale)\n",
        "        return loc, scale\n",
        "\n",
        "\n",
        "class MLPLogits(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim=1, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.out = nn.Linear(hidden_dim, out_dim)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.tanh(self.fc1(x))\n",
        "        h = self.tanh(self.fc2(h))\n",
        "        return self.out(h)\n",
        "\n",
        "\n",
        "class MLPCat(nn.Module):\n",
        "    \"\"\"Categorical logits from an MLP.\"\"\"\n",
        "    def __init__(self, in_dim, num_classes, hidden_dim=64):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.out = nn.Linear(hidden_dim, num_classes)\n",
        "        self.tanh = nn.Tanh()\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = self.tanh(self.fc1(x))\n",
        "        h = self.tanh(self.fc2(h))\n",
        "        return self.out(h)  # logits\n",
        "\n",
        "\n",
        "class ContextCombiner(nn.Module):\n",
        "    \"\"\"\n",
        "    q(z_t | context, g_t):\n",
        "      context = [z_{t-1}, emb(a_{t-1}), v_emb]\n",
        "      g_t = backward RNN message over [x,a,y,v]\n",
        "    \"\"\"\n",
        "    def __init__(self, context_dim, g_dim, z_dim):\n",
        "        super().__init__()\n",
        "        self.lin_context_to_g = nn.Linear(context_dim, g_dim)\n",
        "        self.lin_g_to_loc = nn.Linear(g_dim, z_dim)\n",
        "        self.lin_g_to_scale = nn.Linear(g_dim, z_dim)\n",
        "        self.tanh = nn.Tanh()\n",
        "        self.softplus = nn.Softplus()\n",
        "\n",
        "    def forward(self, context, g_t):\n",
        "        h = 0.5 * (self.tanh(self.lin_context_to_g(context)) + g_t)\n",
        "        loc = self.lin_g_to_loc(h)\n",
        "        scale = self.softplus(self.lin_g_to_scale(h)) + 1e-4\n",
        "        return loc, scale\n",
        "\n",
        "\n",
        "class GatedTransitionZCatAV(nn.Module):\n",
        "    \"\"\"\n",
        "    DMM-style gated transition:\n",
        "      p(z_t | z_{t-1}, a_{t-1}, v) = Normal(loc, scale)\n",
        "    \"\"\"\n",
        "    def __init__(self, z_dim, a_emb_dim, v_emb_dim, transition_dim=64):\n",
        "        super().__init__()\n",
        "        in_dim = z_dim + a_emb_dim + v_emb_dim\n",
        "\n",
        "        self.lin_gate = nn.Linear(in_dim, transition_dim)\n",
        "        self.lin_gate_out = nn.Linear(transition_dim, z_dim)\n",
        "\n",
        "        self.lin_prop = nn.Linear(in_dim, transition_dim)\n",
        "        self.lin_prop_out = nn.Linear(transition_dim, z_dim)\n",
        "\n",
        "        self.lin_sig = nn.Linear(z_dim, z_dim)\n",
        "        self.lin_z_to_loc = nn.Linear(z_dim, z_dim)\n",
        "\n",
        "        # identity init\n",
        "        self.lin_z_to_loc.weight.data = torch.eye(z_dim)\n",
        "        self.lin_z_to_loc.bias.data = torch.zeros(z_dim)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softplus = nn.Softplus()\n",
        "\n",
        "    def forward(self, z_tm1, a_tm1_emb, v_emb):\n",
        "        zav = torch.cat([z_tm1, a_tm1_emb, v_emb], dim=-1)\n",
        "        _gate = self.relu(self.lin_gate(zav))\n",
        "        gate = torch.sigmoid(self.lin_gate_out(_gate))\n",
        "\n",
        "        _prop = self.relu(self.lin_prop(zav))\n",
        "        prop_mean = self.lin_prop_out(_prop)\n",
        "\n",
        "        loc = (1.0 - gate) * self.lin_z_to_loc(z_tm1) + gate * prop_mean\n",
        "        scale = self.softplus(self.lin_sig(self.relu(prop_mean))) + 1e-4\n",
        "        return loc, scale\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# DynVAE model/guide (categorical a) + static covariates v\n",
        "# ============================================================\n",
        "\n",
        "class DynVAE_CatA(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim_x=20,\n",
        "        dim_z=3,\n",
        "        num_actions=6,\n",
        "        a_emb_dim=8,\n",
        "\n",
        "        # static v\n",
        "        dim_v=91,           # <-- requested\n",
        "        v_emb_dim=32,\n",
        "        v_hidden=128,\n",
        "\n",
        "        dim_h=32,\n",
        "        dim_g=32,\n",
        "        transition_dim=64,\n",
        "        hidden_dx=64,\n",
        "        hidden_pa=64,\n",
        "        hidden_y=64,\n",
        "        num_layers=1,\n",
        "        rnn_dropout=0.1,\n",
        "        min_scale_x=0.1,\n",
        "        max_scale_x=0.8,\n",
        "\n",
        "        action_loss_weight=20.0, ######## 10-30\n",
        "        action_class_weights=None,\n",
        "\n",
        "        y_loss_weight=10.0, ######## 10-50\n",
        "        y_class_weights=None,\n",
        "\n",
        "        dim_f=32,\n",
        "        hidden_qtilde=64,\n",
        "        min_scale_z=1e-3,\n",
        "        max_scale_z=5.0,\n",
        "\n",
        "        device=None,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.dim_x = int(dim_x)\n",
        "        self.dim_z = int(dim_z)\n",
        "        self.K = int(num_actions)\n",
        "        self.a_emb_dim = int(a_emb_dim)\n",
        "\n",
        "        self.dim_v = int(dim_v)\n",
        "        self.v_emb_dim = int(v_emb_dim)\n",
        "\n",
        "        self.dim_h = int(dim_h)\n",
        "        self.dim_g = int(dim_g)\n",
        "        self.dim_f = int(dim_f)\n",
        "        self.num_layers = int(num_layers)\n",
        "\n",
        "        self.action_loss_weight = float(action_loss_weight)\n",
        "        if action_class_weights is None:\n",
        "            action_class_weights = torch.ones(self.K)\n",
        "        self.register_buffer(\"action_class_weights\", action_class_weights.float())\n",
        "\n",
        "        self.y_loss_weight = float(y_loss_weight)\n",
        "        if y_class_weights is None:\n",
        "            y_class_weights = torch.ones(2)\n",
        "        self.register_buffer(\"y_class_weights\", y_class_weights.float())\n",
        "\n",
        "        self.device = device if device is not None else torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # v encoder\n",
        "        self.v_enc = MLPEmbed(in_dim=self.dim_v, out_dim=self.v_emb_dim, hidden_dim=v_hidden, dropout=0.0)\n",
        "\n",
        "        # a embedding\n",
        "        self.a_emb = nn.Embedding(self.K, self.a_emb_dim)\n",
        "\n",
        "        # p(a_t | z_t, v)\n",
        "        self.p_a = MLPCat(in_dim=self.dim_z + self.v_emb_dim, num_classes=self.K, hidden_dim=hidden_pa)\n",
        "\n",
        "        # p(z_1)\n",
        "        self.z1_loc = nn.Parameter(torch.zeros(self.dim_z))\n",
        "        self.z1_unconstrained_scale = nn.Parameter(torch.zeros(self.dim_z))\n",
        "        self.softplus = nn.Softplus()\n",
        "\n",
        "        # p(z_t | z_{t-1}, a_{t-1}, v)\n",
        "        self.p_z = GatedTransitionZCatAV(\n",
        "            z_dim=self.dim_z,\n",
        "            a_emb_dim=self.a_emb_dim,\n",
        "            v_emb_dim=self.v_emb_dim,\n",
        "            transition_dim=transition_dim,\n",
        "        )\n",
        "\n",
        "        # p(x_t | h_t)\n",
        "        self.p_x = MLPNormal(in_dim=self.dim_h, out_dim=self.dim_x, hidden_dim=hidden_dx,\n",
        "                             min_scale=min_scale_x, max_scale=max_scale_x)\n",
        "\n",
        "        # p(y | z_T, a_T, v)\n",
        "        self.p_y = MLPLogits(in_dim=self.dim_z + self.K + self.v_emb_dim, out_dim=1, hidden_dim=hidden_y)\n",
        "\n",
        "        # generative GRU\n",
        "        self.gen_rnn = nn.GRU(\n",
        "            input_size=self.dim_z,\n",
        "            hidden_size=self.dim_h,\n",
        "            num_layers=self.num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=0.0,\n",
        "            bidirectional=False,\n",
        "        )\n",
        "\n",
        "        # smoother backward GRU over [x, emb(a), y, v_emb]\n",
        "        self.inf_rnn_bw = nn.GRU(\n",
        "            input_size=self.dim_x + self.a_emb_dim + 1 + self.v_emb_dim,\n",
        "            hidden_size=self.dim_g,\n",
        "            num_layers=self.num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=(rnn_dropout if self.num_layers > 1 else 0.0),\n",
        "            bidirectional=False,\n",
        "        )\n",
        "\n",
        "        # distillation forward GRU over [x, emb(a_{t-1}), v_emb]\n",
        "        self.inf_rnn_fw = nn.GRU(\n",
        "            input_size=self.dim_x + self.a_emb_dim + self.v_emb_dim,\n",
        "            hidden_size=self.dim_f,\n",
        "            num_layers=self.num_layers,\n",
        "            batch_first=True,\n",
        "            dropout=(rnn_dropout if self.num_layers > 1 else 0.0),\n",
        "            bidirectional=False,\n",
        "        )\n",
        "        self.q_tilde = MLPNormal(\n",
        "            in_dim=self.dim_f,\n",
        "            out_dim=self.dim_z,\n",
        "            hidden_dim=hidden_qtilde,\n",
        "            min_scale=min_scale_z,\n",
        "            max_scale=max_scale_z,\n",
        "        )\n",
        "\n",
        "        # q(z_t | [z_{t-1}, emb(a_{t-1}), v_emb], g_t)\n",
        "        self.combiner = ContextCombiner(\n",
        "            context_dim=self.dim_z + self.a_emb_dim + self.v_emb_dim,\n",
        "            g_dim=self.dim_g,\n",
        "            z_dim=self.dim_z,\n",
        "        )\n",
        "\n",
        "        # init states\n",
        "        self.h0 = nn.Parameter(torch.zeros(self.num_layers, 1, self.dim_h))\n",
        "        self.g0 = nn.Parameter(torch.zeros(self.num_layers, 1, self.dim_g))\n",
        "        self.f0 = nn.Parameter(torch.zeros(self.num_layers, 1, self.dim_f))\n",
        "        self.z_q0 = nn.Parameter(torch.zeros(self.dim_z))\n",
        "\n",
        "        self.to(self.device)\n",
        "\n",
        "    def p_z1_params(self, batch_size: int, device: torch.device):\n",
        "        loc = self.z1_loc.unsqueeze(0).expand(batch_size, -1).to(device)\n",
        "        scale = self.softplus(self.z1_unconstrained_scale).unsqueeze(0).expand(batch_size, -1).to(device) + 1e-4\n",
        "        return loc, scale\n",
        "\n",
        "    def model(\n",
        "        self,\n",
        "        batch_x,\n",
        "        batch_a,\n",
        "        batch_y,\n",
        "        batch_v,\n",
        "        batch_mask,\n",
        "        batch_lengths,\n",
        "        annealing_factor: float = 1.0,\n",
        "        distill_weight: float = 0.0,\n",
        "    ):\n",
        "        pyro.module(\"DynVAE_CatA\", self)\n",
        "\n",
        "        x = batch_x.to(self.device)\n",
        "        a = batch_a.to(self.device)\n",
        "        y = batch_y.to(self.device)\n",
        "        v = batch_v.to(self.device)\n",
        "        mask = batch_mask.to(self.device)\n",
        "        lengths = batch_lengths.to(self.device)\n",
        "\n",
        "        B, T_max, _ = x.shape\n",
        "\n",
        "        v_emb = self.v_enc(v)\n",
        "        v_rep = v_emb.unsqueeze(1).expand(B, T_max, self.v_emb_dim)\n",
        "\n",
        "        # distillation forward features f_t (optional)\n",
        "        f = None\n",
        "        distill_weight = float(distill_weight)\n",
        "        if distill_weight > 0.0:\n",
        "            a_shift = torch.zeros_like(a)\n",
        "            a_shift[:, 1:] = a[:, :-1]\n",
        "            a_shift_emb = self.a_emb(a_shift)\n",
        "            w = torch.cat([x, a_shift_emb, v_rep], dim=-1) * mask.unsqueeze(-1).float()\n",
        "\n",
        "            f_init = self.f0.expand(self.num_layers, B, self.dim_f).contiguous()\n",
        "            packed_w = pack_padded_sequence(w, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "            packed_f, _ = self.inf_rnn_fw(packed_w, f_init)\n",
        "            f, _ = pad_packed_sequence(packed_f, batch_first=True, total_length=T_max)\n",
        "            f = f * mask.unsqueeze(-1).float()\n",
        "\n",
        "        h = self.h0.expand(self.num_layers, B, self.dim_h).contiguous()\n",
        "\n",
        "        z_prev: Optional[torch.Tensor] = None\n",
        "        a_prev_idx = torch.zeros(B, dtype=torch.long, device=self.device)\n",
        "        a_prev_emb = self.a_emb(a_prev_idx)\n",
        "\n",
        "        beta = max(float(annealing_factor), 1e-8)\n",
        "\n",
        "        with pyro.plate(\"batch\", B):\n",
        "            for t in pyro.markov(range(T_max)):\n",
        "                mask_t = mask[:, t]\n",
        "\n",
        "                if t == 0:\n",
        "                    z_loc, z_scale = self.p_z1_params(B, self.device)\n",
        "                else:\n",
        "                    z_loc, z_scale = self.p_z(z_prev, a_prev_emb, v_emb)\n",
        "\n",
        "                with poutine.scale(scale=beta):\n",
        "                    z_t = pyro.sample(f\"z_{t+1}\", dist.Normal(z_loc, z_scale).to_event(1).mask(mask_t))\n",
        "\n",
        "                if f is not None:\n",
        "                    zt_loc_d, zt_scale_d = self.q_tilde(f[:, t, :])\n",
        "                    log_qtilde = dist.Normal(zt_loc_d, zt_scale_d).log_prob(z_t.detach()).sum(-1)\n",
        "                    pyro.factor(f\"distill_{t+1}\", distill_weight * mask_t.float() * log_qtilde)\n",
        "\n",
        "                # a_t observed\n",
        "                a_logits = self.p_a(torch.cat([z_t, v_emb], dim=-1))\n",
        "                w_t = self.action_class_weights[a[:, t]].clamp_min(1e-6)\n",
        "                w_t = torch.where(mask_t, w_t, torch.ones_like(w_t))\n",
        "                with poutine.scale(scale=self.action_loss_weight * w_t):\n",
        "                    pyro.sample(f\"a_{t+1}\", dist.Categorical(logits=a_logits).mask(mask_t), obs=a[:, t])\n",
        "\n",
        "                # update h only on valid\n",
        "                out, h_new = self.gen_rnn(z_t.unsqueeze(1), h)\n",
        "                h = torch.where(mask_t.view(1, B, 1), h_new, h)\n",
        "                h_t = out[:, 0, :]\n",
        "\n",
        "                # x_t observed\n",
        "                x_loc, x_scale = self.p_x(h_t)\n",
        "                pyro.sample(f\"x_{t+1}\", dist.Normal(x_loc, x_scale).to_event(1).mask(mask_t), obs=x[:, t, :])\n",
        "\n",
        "                # last valid z/a\n",
        "                if t == 0:\n",
        "                    z_prev = z_t\n",
        "                else:\n",
        "                    z_prev = torch.where(mask_t.unsqueeze(-1), z_t, z_prev)\n",
        "                a_prev_idx = torch.where(mask_t, a[:, t], a_prev_idx)\n",
        "                a_prev_emb = self.a_emb(a_prev_idx)\n",
        "\n",
        "            # y observed\n",
        "            aT_onehot = F.one_hot(a_prev_idx, num_classes=self.K).float()\n",
        "            y_logits = self.p_y(torch.cat([z_prev, aT_onehot, v_emb], dim=-1))\n",
        "            y_idx = y.view(-1).long().clamp(0, 1)\n",
        "            w_y = self.y_class_weights[y_idx].clamp_min(1e-6)\n",
        "            with poutine.scale(scale=self.y_loss_weight * w_y):\n",
        "                pyro.sample(\"y\", dist.Bernoulli(logits=y_logits).to_event(1), obs=y)\n",
        "\n",
        "    def guide(\n",
        "        self,\n",
        "        batch_x,\n",
        "        batch_a,\n",
        "        batch_y,\n",
        "        batch_v,\n",
        "        batch_mask,\n",
        "        batch_lengths,\n",
        "        annealing_factor: float = 1.0,\n",
        "        distill_weight: float = 0.0,\n",
        "    ):\n",
        "        pyro.module(\"DynVAE_CatA\", self)\n",
        "\n",
        "        x = batch_x.to(self.device)\n",
        "        a = batch_a.to(self.device)\n",
        "        y = batch_y.to(self.device)\n",
        "        v = batch_v.to(self.device)\n",
        "        mask = batch_mask.to(self.device)\n",
        "        lengths = batch_lengths.to(self.device)\n",
        "\n",
        "        B, T_max, _ = x.shape\n",
        "\n",
        "        v_emb = self.v_enc(v)\n",
        "        v_rep = v_emb.unsqueeze(1).expand(B, T_max, self.v_emb_dim)\n",
        "\n",
        "        a_emb_t = self.a_emb(a)\n",
        "        y_rep = y.unsqueeze(1).expand(B, T_max, 1)\n",
        "        u = torch.cat([x, a_emb_t, y_rep, v_rep], dim=-1) * mask.unsqueeze(-1).float()\n",
        "\n",
        "        u_rev = reverse_padded_sequence(u, lengths)\n",
        "        g_init = self.g0.expand(self.num_layers, B, self.dim_g).contiguous()\n",
        "\n",
        "        packed = pack_padded_sequence(u_rev, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        packed_out, _ = self.inf_rnn_bw(packed, g_init)\n",
        "        g_rev, _ = pad_packed_sequence(packed_out, batch_first=True, total_length=T_max)\n",
        "        g = reverse_padded_sequence(g_rev, lengths) * mask.unsqueeze(-1).float()\n",
        "\n",
        "        z_prev = self.z_q0.expand(B, self.dim_z)\n",
        "        a_prev_idx = torch.zeros(B, dtype=torch.long, device=self.device)\n",
        "        a_prev_emb = self.a_emb(a_prev_idx)\n",
        "\n",
        "        beta = max(float(annealing_factor), 1e-8)\n",
        "\n",
        "        with pyro.plate(\"batch\", B):\n",
        "            for t in pyro.markov(range(T_max)):\n",
        "                mask_t = mask[:, t]\n",
        "                context = torch.cat([z_prev, a_prev_emb, v_emb], dim=-1)\n",
        "                z_loc, z_scale = self.combiner(context, g[:, t, :])\n",
        "                with poutine.scale(scale=beta):\n",
        "                    z_t = pyro.sample(f\"z_{t+1}\", dist.Normal(z_loc, z_scale).to_event(1).mask(mask_t))\n",
        "\n",
        "                z_prev = torch.where(mask_t.unsqueeze(-1), z_t, z_prev)\n",
        "                a_prev_idx = torch.where(mask_t, a[:, t], a_prev_idx)\n",
        "                a_prev_emb = self.a_emb(a_prev_idx)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def distill_kl_sum_count(self, x, a, y, v, mask, lengths, use_mean_z_prev: bool = True):\n",
        "        \"\"\"KL(q_phi || q_tilde) summed over valid time steps.\"\"\"\n",
        "        self.eval()\n",
        "        x = x.to(self.device)\n",
        "        a = a.to(self.device)\n",
        "        y = y.to(self.device)\n",
        "        v = v.to(self.device)\n",
        "        mask = mask.to(self.device)\n",
        "        lengths = lengths.to(self.device)\n",
        "\n",
        "        B, T_max, _ = x.shape\n",
        "        v_emb = self.v_enc(v)\n",
        "        v_rep = v_emb.unsqueeze(1).expand(B, T_max, self.v_emb_dim)\n",
        "\n",
        "        # g_t\n",
        "        a_emb_t = self.a_emb(a)\n",
        "        y_rep = y.unsqueeze(1).expand(B, T_max, 1)\n",
        "        u = torch.cat([x, a_emb_t, y_rep, v_rep], dim=-1) * mask.unsqueeze(-1).float()\n",
        "\n",
        "        u_rev = reverse_padded_sequence(u, lengths)\n",
        "        g_init = self.g0.expand(self.num_layers, B, self.dim_g).contiguous()\n",
        "        packed = pack_padded_sequence(u_rev, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        packed_out, _ = self.inf_rnn_bw(packed, g_init)\n",
        "        g_rev, _ = pad_packed_sequence(packed_out, batch_first=True, total_length=T_max)\n",
        "        g = reverse_padded_sequence(g_rev, lengths) * mask.unsqueeze(-1).float()\n",
        "\n",
        "        # f_t\n",
        "        a_shift = torch.zeros_like(a)\n",
        "        a_shift[:, 1:] = a[:, :-1]\n",
        "        a_shift_emb = self.a_emb(a_shift)\n",
        "        w = torch.cat([x, a_shift_emb, v_rep], dim=-1) * mask.unsqueeze(-1).float()\n",
        "\n",
        "        f_init = self.f0.expand(self.num_layers, B, self.dim_f).contiguous()\n",
        "        packed_w = pack_padded_sequence(w, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
        "        packed_f, _ = self.inf_rnn_fw(packed_w, f_init)\n",
        "        f, _ = pad_packed_sequence(packed_f, batch_first=True, total_length=T_max)\n",
        "        f = f * mask.unsqueeze(-1).float()\n",
        "\n",
        "        loc_tilde, scale_tilde = self.q_tilde(f.reshape(-1, self.dim_f))\n",
        "        loc_tilde = loc_tilde.reshape(B, T_max, self.dim_z)\n",
        "        scale_tilde = scale_tilde.reshape(B, T_max, self.dim_z).clamp_min(1e-6)\n",
        "\n",
        "        z_prev = self.z_q0.expand(B, self.dim_z)\n",
        "        a_prev_idx = torch.zeros(B, dtype=torch.long, device=self.device)\n",
        "        a_prev_emb = self.a_emb(a_prev_idx)\n",
        "\n",
        "        kl_sum = x.new_zeros(())\n",
        "        count = x.new_zeros(())\n",
        "\n",
        "        for t in range(T_max):\n",
        "            context = torch.cat([z_prev, a_prev_emb, v_emb], dim=-1)\n",
        "            loc_phi, scale_phi = self.combiner(context, g[:, t, :])\n",
        "            scale_phi = scale_phi.clamp_min(1e-6)\n",
        "\n",
        "            q_phi_t = td.Normal(loc_phi, scale_phi)\n",
        "            q_tilde_t = td.Normal(loc_tilde[:, t, :], scale_tilde[:, t, :])\n",
        "            kl_bt = td.kl_divergence(q_phi_t, q_tilde_t).sum(-1)\n",
        "\n",
        "            m = mask[:, t].float()\n",
        "            kl_sum = kl_sum + (kl_bt * m).sum()\n",
        "            count = count + m.sum()\n",
        "\n",
        "            z_next = loc_phi if use_mean_z_prev else q_phi_t.rsample()\n",
        "            z_prev = torch.where(mask[:, t].unsqueeze(-1), z_next, z_prev)\n",
        "            a_prev_idx = torch.where(mask[:, t], a[:, t], a_prev_idx)\n",
        "            a_prev_emb = self.a_emb(a_prev_idx)\n",
        "\n",
        "        return kl_sum.detach(), count.detach()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# KL annealing schedule\n",
        "# ============================================================\n",
        "\n",
        "def make_anneal_fn(steps_per_epoch, epochs, kl_start_epoch=0, ramp_fraction_of_remaining=0.5, eps=1e-8):\n",
        "    total_steps = int(steps_per_epoch * epochs)\n",
        "    start_step = int(steps_per_epoch * kl_start_epoch)\n",
        "    remaining = max(0, total_steps - start_step)\n",
        "    ramp_steps = max(1, int(ramp_fraction_of_remaining * remaining))\n",
        "\n",
        "    def anneal(step):\n",
        "        if step <= start_step:\n",
        "            return float(eps)\n",
        "        s = step - start_step\n",
        "        return float(max(eps, min(1.0, s / float(ramp_steps))))\n",
        "\n",
        "    info = {\n",
        "        \"total_steps\": total_steps,\n",
        "        \"start_step\": start_step,\n",
        "        \"ramp_steps\": ramp_steps,\n",
        "        \"ramp_end_step\": start_step + ramp_steps,\n",
        "    }\n",
        "    return anneal, info\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Class weights (loader: x,a,y,v,lengths,mask)\n",
        "# ============================================================\n",
        "\n",
        "def compute_action_class_weights(train_loader, K: int):\n",
        "    counts = torch.zeros(K)\n",
        "    for _x, a, _y, _v, lengths, mask in train_loader:\n",
        "        counts += torch.bincount(a[mask].flatten(), minlength=K).float()\n",
        "    freq = counts / counts.sum().clamp_min(1.0)\n",
        "    w = 1.0 / (freq + 1e-6)\n",
        "    w = w / w.mean().clamp_min(1e-6)\n",
        "    return w\n",
        "\n",
        "\n",
        "def compute_binary_class_weights(train_loader):\n",
        "    counts = torch.zeros(2)\n",
        "    for _x, _a, y, _v, lengths, mask in train_loader:\n",
        "        y_idx = y.view(-1).long().clamp(0, 1)\n",
        "        counts += torch.bincount(y_idx, minlength=2).float()\n",
        "    freq = counts / counts.sum().clamp_min(1.0)\n",
        "    w = 1.0 / (freq + 1e-6)\n",
        "    w = w / w.mean().clamp_min(1e-6)\n",
        "    return w\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Training\n",
        "# ============================================================\n",
        "\n",
        "def train_svi(\n",
        "    train_loader,\n",
        "    model: DynVAE_CatA,\n",
        "    test_loader=None,\n",
        "    epochs=80,\n",
        "    lr=1e-3,\n",
        "    kl_start_epoch=2,\n",
        "    seed=0,\n",
        "    log_every=1,\n",
        "    num_particles=1,\n",
        "    distill_weight=1.0,\n",
        "):\n",
        "    pyro.set_rng_seed(seed)\n",
        "    pyro.clear_param_store()\n",
        "\n",
        "    optim = ClippedAdam({\"lr\": lr, \"clip_norm\": 5.0})\n",
        "    loss_fn = Trace_ELBO(num_particles=num_particles, vectorize_particles=(num_particles > 1))\n",
        "    svi = SVI(model.model, model.guide, optim, loss=loss_fn)\n",
        "\n",
        "    steps_per_epoch = len(train_loader)\n",
        "    anneal_fn, info = make_anneal_fn(\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        epochs=epochs,\n",
        "        kl_start_epoch=kl_start_epoch,\n",
        "        ramp_fraction_of_remaining=0.5,\n",
        "        eps=1e-8,\n",
        "    )\n",
        "    print(\n",
        "        f\"[info] steps/epoch={steps_per_epoch}, total_steps={info['total_steps']}, \"\n",
        "        f\"kl_start_epoch={kl_start_epoch}, start_step={info['start_step']}, \"\n",
        "        f\"ramp_steps={info['ramp_steps']}, ramp_end_step={info['ramp_end_step']}\"\n",
        "    )\n",
        "\n",
        "    def _print_header():\n",
        "        print(\"epoch |  train loss/t | test@1.0 loss/t |  beta || distill KL/t (q_phi || q_tilde)\")\n",
        "        print(\"----- | ------------ | -------------- | ----- || ---------------------------\")\n",
        "\n",
        "    step = 0\n",
        "    beta = 1.0\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        model.train()\n",
        "        for x, a, y, v, lengths, mask in train_loader:\n",
        "            step += 1\n",
        "            beta = anneal_fn(step)\n",
        "            svi.step(x, a, y, v, mask, lengths, annealing_factor=beta, distill_weight=distill_weight)\n",
        "\n",
        "        model.eval()\n",
        "        train_elbo = 0.0\n",
        "        train_T = 0.0\n",
        "        distill_kl_sum = 0.0\n",
        "        distill_kl_count = 0.0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for x, a, y, v, lengths, mask in train_loader:\n",
        "                train_elbo += float(\n",
        "                    svi.evaluate_loss(x, a, y, v, mask, lengths, annealing_factor=beta, distill_weight=0.0)\n",
        "                )\n",
        "                train_T += float(lengths.sum().item())\n",
        "\n",
        "                if distill_weight > 0.0:\n",
        "                    kl_sum, kl_count = model.distill_kl_sum_count(x, a, y, v, mask, lengths)\n",
        "                    distill_kl_sum += float(kl_sum.item())\n",
        "                    distill_kl_count += float(kl_count.item())\n",
        "\n",
        "        train_loss_per_t = train_elbo / max(train_T, 1.0)\n",
        "        distill_kl_per_t = distill_kl_sum / max(distill_kl_count, 1.0)\n",
        "\n",
        "        test_full_t = None\n",
        "        if test_loader is not None:\n",
        "            total_full = 0.0\n",
        "            total_full_T = 0.0\n",
        "            with torch.no_grad():\n",
        "                for x, a, y, v, lengths, mask in test_loader:\n",
        "                    total_full += float(\n",
        "                        svi.evaluate_loss(x, a, y, v, mask, lengths, annealing_factor=1.0, distill_weight=0.0)\n",
        "                    )\n",
        "                    total_full_T += float(lengths.sum().item())\n",
        "            test_full_t = total_full / max(total_full_T, 1.0)\n",
        "\n",
        "        if epoch % log_every == 0:\n",
        "            if epoch == 1 or epoch % 10 == 0:\n",
        "                _print_header()\n",
        "            if test_loader is None:\n",
        "                print(f\"{epoch:5d} | {train_loss_per_t:12.4f} | {'-':14s} | {beta:5.3f} || {distill_kl_per_t:27.4f}\")\n",
        "            else:\n",
        "                print(f\"{epoch:5d} | {train_loss_per_t:12.4f} | {test_full_t:14.4f} | {beta:5.3f} || {distill_kl_per_t:27.4f}\")\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Dataset / loaders (no z)\n",
        "# ============================================================\n",
        "\n",
        "class TrajectoryDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Each item:\n",
        "      x: (T,dim_x) float\n",
        "      a: (T,) long\n",
        "      y: (1,) float\n",
        "      v: (dim_v,) float\n",
        "    \"\"\"\n",
        "    def __init__(self, trajectories: Sequence[Dict[str, Any]]):\n",
        "        self.trajs = list(trajectories)\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.trajs)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        tr = self.trajs[idx]\n",
        "        x = torch.as_tensor(tr[\"x\"], dtype=torch.float32)\n",
        "        a = torch.as_tensor(tr[\"a\"], dtype=torch.long)\n",
        "        y = torch.as_tensor(tr[\"y\"], dtype=torch.float32).view(1)\n",
        "        v = torch.as_tensor(tr[\"v\"], dtype=torch.float32)\n",
        "        return x, a, y, v\n",
        "\n",
        "\n",
        "def collate_trajectories(batch: List[Tuple[torch.Tensor, ...]]):\n",
        "    xs, aas, ys, vs = zip(*batch)\n",
        "\n",
        "    lengths = torch.tensor([x.shape[0] for x in xs], dtype=torch.long)\n",
        "    T_max = int(lengths.max().item())\n",
        "\n",
        "    x_pad = pad_sequence(xs, batch_first=True, padding_value=0.0)  # (B,T,dim_x)\n",
        "    a_pad = pad_sequence(aas, batch_first=True, padding_value=0)   # (B,T)\n",
        "\n",
        "    mask = (torch.arange(T_max)[None, :] < lengths[:, None])       # (B,T) bool\n",
        "\n",
        "    v_stack = torch.stack(vs, dim=0)                               # (B,dim_v)\n",
        "    y_stack = torch.stack(ys, dim=0)                               # (B,1)\n",
        "\n",
        "    return x_pad, a_pad, y_stack, v_stack, lengths, mask\n",
        "\n",
        "\n",
        "def make_toy_dataset(\n",
        "    n: int,\n",
        "    dim_x: int,\n",
        "    dim_v: int,\n",
        "    num_actions: int,\n",
        "    min_T: int = 10,\n",
        "    max_T: int = 30,\n",
        "    seed: int = 0,\n",
        ") -> List[Dict[str, Any]]:\n",
        "    rng = np.random.default_rng(seed)\n",
        "    trajs: List[Dict[str, Any]] = []\n",
        "    for _ in range(n):\n",
        "        T = int(rng.integers(min_T, max_T + 1))\n",
        "        x = rng.normal(size=(T, dim_x)).astype(np.float32)\n",
        "        a = rng.integers(0, num_actions, size=(T,), dtype=np.int64)\n",
        "        y = rng.integers(0, 2, size=(1,), dtype=np.int64).astype(np.float32)\n",
        "        v = rng.normal(size=(dim_v,)).astype(np.float32)\n",
        "        trajs.append({\"x\": x, \"a\": a, \"y\": y, \"v\": v})\n",
        "    return trajs\n",
        "\n",
        "\n",
        "def build_loaders_from_npz(npz_path: str, batch_size: int, num_workers: int = 0):\n",
        "    \"\"\"\n",
        "    Expected arrays in npz:\n",
        "      - x: (N,T,dim_x) padded\n",
        "      - a: (N,T) padded\n",
        "      - y: (N,1)\n",
        "      - v: (N,dim_v)\n",
        "      - lengths: (N,)\n",
        "    \"\"\"\n",
        "    data = np.load(npz_path)\n",
        "    x = torch.tensor(data[\"x\"], dtype=torch.float32)\n",
        "    a = torch.tensor(data[\"a\"], dtype=torch.long)\n",
        "    y = torch.tensor(data[\"y\"], dtype=torch.float32)\n",
        "    v = torch.tensor(data[\"v\"], dtype=torch.float32)\n",
        "    lengths = torch.tensor(data[\"lengths\"], dtype=torch.long)\n",
        "\n",
        "    T_max = x.shape[1]\n",
        "    mask = (torch.arange(T_max)[None, :] < lengths[:, None])\n",
        "\n",
        "    class _Padded(Dataset):\n",
        "        def __len__(self): return x.shape[0]\n",
        "        def __getitem__(self, i):\n",
        "            return x[i], a[i], y[i], v[i], lengths[i], mask[i]\n",
        "\n",
        "    ds = _Padded()\n",
        "    n_test = max(1, int(0.2 * len(ds)))\n",
        "    n_train = len(ds) - n_test\n",
        "    train_ds, test_ds = random_split(ds, [n_train, n_test], generator=torch.Generator().manual_seed(0))\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Main (notebook-safe argparse)\n",
        "# ============================================================\n",
        "\n",
        "def main(argv=None):\n",
        "    parser = argparse.ArgumentParser(add_help=True)\n",
        "    parser.add_argument(\"--epochs\", type=int, default=1000)\n",
        "    parser.add_argument(\"--batch_size\", type=int, default=64)\n",
        "    parser.add_argument(\"--lr\", type=float, default=1e-3)\n",
        "    parser.add_argument(\"--kl_start_epoch\", type=int, default=2)\n",
        "    parser.add_argument(\"--seed\", type=int, default=0)\n",
        "\n",
        "    # NOTEBOOK SAFE: ignore extra jupyter args like -f kernel.json\n",
        "    if argv is None:\n",
        "        args, _ = parser.parse_known_args()\n",
        "    else:\n",
        "        args, _ = parser.parse_known_args(argv)\n",
        "\n",
        "    set_all_seeds(args.seed)\n",
        "\n",
        "    # match your setup (edit if needed)\n",
        "    dim_x = 39\n",
        "    dim_z = 6\n",
        "    num_actions = 7\n",
        "    dim_v = 91\n",
        "\n",
        "    train_loader, test_loader, report = create_loaders(\n",
        "        use_incremental_for_cumulative_x=True,\n",
        "        pid_width=None,\n",
        "        normalize_x=True,\n",
        "        normalize_v=True,\n",
        "        scale_nonneg_only=True,\n",
        "        plot_length_hist=True,\n",
        "        asa_encoding=\"onehot\",  # or \"ordinal\"\n",
        "    )\n",
        "\n",
        "    a_w = compute_action_class_weights(train_loader, K=num_actions)\n",
        "    y_w = compute_binary_class_weights(train_loader)\n",
        "\n",
        "    model = DynVAE_CatA(\n",
        "        dim_x=dim_x,\n",
        "        dim_z=dim_z,\n",
        "        num_actions=num_actions,\n",
        "        a_emb_dim=10,\n",
        "\n",
        "        dim_v=dim_v,\n",
        "        v_emb_dim=5,\n",
        "        v_hidden=128,\n",
        "\n",
        "        dim_h=64,\n",
        "        dim_g=64,\n",
        "        transition_dim=64,\n",
        "        hidden_dx=64,\n",
        "        hidden_pa=64,\n",
        "        hidden_y=64,\n",
        "        num_layers=1,\n",
        "        rnn_dropout=0.1,\n",
        "        min_scale_x=0.10,\n",
        "        max_scale_x=0.80,\n",
        "\n",
        "        action_loss_weight=10.0,\n",
        "        action_class_weights=a_w,\n",
        "\n",
        "        y_loss_weight=10.0,\n",
        "        y_class_weights=y_w,\n",
        "\n",
        "        dim_f=64,\n",
        "        hidden_qtilde=64,\n",
        "        min_scale_z=1e-3,\n",
        "        max_scale_z=5.0,\n",
        "    )\n",
        "\n",
        "    train_svi(\n",
        "        train_loader=train_loader,\n",
        "        model=model,\n",
        "        test_loader=test_loader,\n",
        "        epochs=args.epochs,\n",
        "        lr=args.lr,\n",
        "        kl_start_epoch=args.kl_start_epoch,\n",
        "        seed=args.seed,\n",
        "        log_every=1,\n",
        "        num_particles=1,\n",
        "        distill_weight=1.0,\n",
        "    )\n",
        "\n",
        "\n",
        "# If running as a script, this works; in notebooks, just call main([])\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
